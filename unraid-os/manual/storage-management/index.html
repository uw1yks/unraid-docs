<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-unraid-os/manual/storage-management" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.4.0">
<title data-rh="true">Storage Management | Unraid Docs</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docs.unraid.net/img/meta-unraid.png"><meta data-rh="true" name="twitter:image" content="https://docs.unraid.net/img/meta-unraid.png"><meta data-rh="true" property="og:url" content="https://docs.unraid.net/unraid-os/manual/storage-management/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh"><meta data-rh="true" property="og:locale:alternate" content="es"><meta data-rh="true" property="og:locale:alternate" content="fr"><meta data-rh="true" property="og:locale:alternate" content="de"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="theme-color" content="#242526" media="(prefers-color-scheme: dark)"><meta data-rh="true" name="theme-color" content="#ffffff" media="(prefers-color-scheme: light)"><meta data-rh="true" name="color-scheme" content="dark light"><meta data-rh="true" name="keywords" content="Unraid, server, storage, NAS, Docker, virtualization, array, parity, data protection, file sharing, plugins, management, GUI, disk management, caching, SSD, disk encryption, security, RAID, network configuration, backups, media server, transcoding, monitoring, VMs, GPU passthrough, hardware compatibility"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Storage Management | Unraid Docs"><meta data-rh="true" name="description" content="To assign devices to"><meta data-rh="true" property="og:description" content="To assign devices to"><link data-rh="true" rel="icon" href="/img/favicon.svg"><link data-rh="true" rel="canonical" href="https://docs.unraid.net/unraid-os/manual/storage-management/"><link data-rh="true" rel="alternate" href="https://docs.unraid.net/unraid-os/manual/storage-management/" hreflang="en"><link data-rh="true" rel="alternate" href="https://docs.unraid.net/zh/unraid-os/manual/storage-management/" hreflang="zh"><link data-rh="true" rel="alternate" href="https://docs.unraid.net/es/unraid-os/manual/storage-management/" hreflang="es"><link data-rh="true" rel="alternate" href="https://docs.unraid.net/fr/unraid-os/manual/storage-management/" hreflang="fr"><link data-rh="true" rel="alternate" href="https://docs.unraid.net/de/unraid-os/manual/storage-management/" hreflang="de"><link data-rh="true" rel="alternate" href="https://docs.unraid.net/unraid-os/manual/storage-management/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://JUYLFQHE7W-dsn.algolia.net" crossorigin="anonymous"><link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CZENQ1ZPEH"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-CZENQ1ZPEH",{anonymize_ip:!0})</script>


<link rel="search" type="application/opensearchdescription+xml" title="Unraid Docs" href="/opensearch.xml"><link rel="stylesheet" href="/assets/css/styles.3916f753.css">
<script src="/assets/js/runtime~main.71c18b2a.js" defer="defer"></script>
<script src="/assets/js/main.9f125ea7.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();null!==e?t(e):window.matchMedia("(prefers-color-scheme: dark)").matches?t("dark"):window.matchMedia("(prefers-color-scheme: light)").matches?t("light"):t("dark")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/un-mark-gradient.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE" style="width:30px"><img src="/img/un-mark-gradient.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU" style="width:30px"></div><b class="navbar__title text--truncate">Unraid Docs</b></a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link" href="/unraid-os/manual/storage-management/">More</a><ul class="dropdown__menu"><li><a href="https://unraid.net" target="_blank" rel="noopener noreferrer" class="dropdown__link">Unraid Home<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://forums.unraid.net" target="_blank" rel="noopener noreferrer" class="dropdown__link">Forums<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://github.com/unraid/docs" target="_blank" rel="noopener noreferrer" class="dropdown__link">Docs Github<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link" href="/unraid-os/manual/storage-management/"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/unraid-os/manual/storage-management/" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li><li><a href="/zh/unraid-os/manual/storage-management/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh">中文</a></li><li><a href="/es/unraid-os/manual/storage-management/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="es">Español</a></li><li><a href="/fr/unraid-os/manual/storage-management/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="fr">Français</a></li><li><a href="/de/unraid-os/manual/storage-management/" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="de">Deutsch</a></li></ul></div><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently dark mode)" aria-label="Switch between dark and light mode (currently dark mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/">Home</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/category/unraid-os/">Unraid OS</a><button aria-label="Collapse sidebar category &#x27;Unraid OS&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/unraid-os/overview/what-is-unraid/">Overview</a><button aria-label="Expand sidebar category &#x27;Overview&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/category/release-notes/">Release Notes</a><button aria-label="Expand sidebar category &#x27;Release Notes&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/unraid-os/getting-started/">Getting started</a><button aria-label="Expand sidebar category &#x27;Getting started&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/category/manual/">Manual</a><button aria-label="Collapse sidebar category &#x27;Manual&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/unraid-os/manual/shares/">Shares</a><button aria-label="Expand sidebar category &#x27;Shares&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/unraid-os/manual/users/">User management</a><button aria-label="Expand sidebar category &#x27;User management&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/unraid-os/manual/storage-management/">Storage Management</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/unraid-os/manual/docker-management/">Docker Management</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/unraid-os/manual/vm/vm-management/">VM Management</a><button aria-label="Expand sidebar category &#x27;VM Management&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/unraid-os/manual/applications/">Community Applications</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/unraid-os/manual/security/">Security</a><button aria-label="Expand sidebar category &#x27;Security&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/unraid-os/manual/multi-language/">Multi-language support</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/unraid-os/manual/additional-settings/">Additional Settings</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/unraid-os/manual/changing-the-flash-device/">Changing the flash device</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/unraid-os/manual/tools/">Tools</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/unraid-os/manual/upgrade-instructions/">Upgrading Unraid</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/category/zfs/">ZFS</a><button aria-label="Expand sidebar category &#x27;ZFS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/category/guides/">Guides</a><button aria-label="Expand sidebar category &#x27;Guides&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/unraid-os/troubleshooting/">Troubleshooting</a><button aria-label="Expand sidebar category &#x27;Troubleshooting&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/category/faq/">FAQ</a><button aria-label="Expand sidebar category &#x27;FAQ&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unraid-os/download_list/">Download Archive</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/account/">Unraid.net Account</a><button aria-label="Expand sidebar category &#x27;Unraid.net Account&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/category/unraid-connect/">Unraid Connect</a><button aria-label="Expand sidebar category &#x27;Unraid Connect&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/category/legacy-documentation/">Legacy Documentation</a><button aria-label="Expand sidebar category &#x27;Legacy Documentation&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/category/contribute/">Contribute</a><button aria-label="Expand sidebar category &#x27;Contribute&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/category/unraid-os/"><span itemprop="name">Unraid OS</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/category/manual/"><span itemprop="name">Manual</span></a><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Storage Management</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Storage Management</h1>
<p><img decoding="async" loading="lazy" src="/assets/images/Configuringarray1-b563b7a09206ee11573eb77ad400c45c.png" width="1648" height="859" class="img_ev3q"></p>
<p>To assign devices to
the array and/or cache, first login to the server&#x27;s WebGUI. Click on
the <strong>Main</strong> tab and select the devices to assign to slots for parity,
data, and cache disks. Assigning devices to Unraid is easy! Just
remember these guidelines:</p>
<ul>
<li>
<p><strong>Always pick the largest storage device available to act as your
parity device(s)</strong>. When expanding your array in the future (adding
more devices to data disk slots), you cannot assign a data disk that
is larger than your parity device(s). For this reason, it is highly
recommended to purchase the largest HDD available for use as your
initial parity device, so future expansions aren&#x27;t limited to small
device sizes. If assigning dual parity disks, your two parity disks
can vary in size, but the same rule holds true that no disk in the
array can be larger than your smallest parity device.</p>
</li>
<li>
<p><strong>SSD support in the array is experimental</strong>. Some SSDs may not be
ideal for use in the array due to how TRIM/Discard may be
implemented. Using SSDs as data/parity devices may have
unexpected/undesirable results. This does NOT apply to the cache /
cache pool. Most modern SSDs will work fine in the array, and even
NVMe devices are now supported, but know that until these devices
are in wider use, we only have limited testing experience using them
in this setting.</p>
</li>
<li>
<p><strong>Using a cache will improve array performance</strong>. It does this by
redirecting write operations to a dedicated disk (or pool of disks
in Unraid 6) and moves that data to the array on a schedule that you
define (by default, once per day at 3:40AM). Data written to the
cache is still presented through your user shares, making use of
this function completely transparent.</p>
</li>
<li>
<p><strong>Creating a cache-pool adds protection for cached data</strong>. If you
only assign one cache device to the system, data residing there
before being moved to the array on a schedule is not protected from
data loss. To ensure data remains protected at all times (both on
data and cache disks), you must assign more than one device to the
cache function, creating what is called a cache-pool. Cache pools
can be expanded on demand, similar to the array.</p>
</li>
<li>
<p><strong>SSD-based cache devices are ideal for applications and virtual
machines</strong>. Apps and VMs benefit from SSDs as they can leverage
their raw IO potential to perform faster when interacting with them.
Use SSDs in a cache pool for the ultimate combination of
functionality, performance, and protection.</p>
</li>
<li>
<p><strong>Encryption is disabled by default</strong>. If you wish to use this
feature on your system, you can do so by adjusting the file system
for the devices you wish to have encrypted. Click on each disk you
wish to have encrypted and toggle the filesystem to one of the
encrypted options. Note, however, that using encryption can
complicate recovering from certain types of failure so do not use
this feature just because it is available if you have no need for
it.</p>
</li>
</ul>
<p>Unraid recognizes disks by their serial number (and size). This means
that it is possible to move drives between SATA ports without having to
make any changes in drive assignments. This can be useful for
troubleshooting if you ever suspect there may be a hardware-related
issue such as a bad port or a think a power or SATA cable may be
suspect.</p>
<p><em>NOTE: Your array will not start if you assign or attach more devices
than your license key allows.</em></p>
<h1>Starting and stopping the array</h1>
<p>Normally following system boot up the array (complete set of disks) is
automatically started (brought on-line and exported as a set of shares).
But if there&#x27;s been a change in disk configuration, such as a new disk
added, the array is left stopped so that you can confirm the
configuration is correct. This means that any time you have made a disk
configuration change you must log in to the WebGUI and manually start
the array. When you wish to make changes to disks in your array, you
will need to stop the array to do this. Stopping the array means all of
your applications/services are stopped, and your storage devices are
unmounted, making all data and applications unavailable until you once
again start the array. To start or stop the array, perform the following
steps:</p>
<ol>
<li>Log into the Unraid WebGUI using a browser (e.g. <code>http://tower</code>;
<code>http://tower.local</code> from Mac)</li>
<li>Click on <strong>Main</strong></li>
<li>Go to the <em>Array Operation</em> section</li>
<li>Click <strong>Start</strong> or <strong>Stop</strong> (you may first need to click the &quot;Yes I
want to do this&quot; checkbox)</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="help-i-cant-start-my-array">Help! I can&#x27;t start my array!<a class="hash-link" aria-label="Direct link to Help! I can&#x27;t start my array!" title="Direct link to Help! I can&#x27;t start my array!" href="/unraid-os/manual/storage-management/#help-i-cant-start-my-array">​</a></h2>
<p>If the array can&#x27;t be started, it may be for one of a few reasons which
will be reported under the <em>Array Operation</em> section:</p>
<ul>
<li>Too many wrong and/or missing disks</li>
<li>Too many attached devices</li>
<li>Invalid or missing registration key</li>
<li>Cannot contact key-server</li>
<li>This Unraid Server OS release has been withdrawn</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="too-many-disks-missing-from-the-array">Too many disks missing from the array<a class="hash-link" aria-label="Direct link to Too many disks missing from the array" title="Direct link to Too many disks missing from the array" href="/unraid-os/manual/storage-management/#too-many-disks-missing-from-the-array">​</a></h3>
<p><img decoding="async" loading="lazy" alt="indication that you have too many devices missing or incorrectly assigned" src="/assets/images/Toomanywrong-323c00f4a6565922be138b305fe75893.png" width="1590" height="166" class="img_ev3q"></p>
<p>If you have no parity disks, this message won&#x27;t appear.</p>
<p>If you have a single parity disk, you can only have up to one disk
missing and still start the array, as parity will then help simulate the
contents of the missing disk until you can replace it.</p>
<p>If you have two parity disks, you can have up to two disks missing and
still start the array.</p>
<p>If more than two disks are missing / wrong due to a catastrophic
failure, you will need to perform the New Config procedure.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="too-many-attached-devices">Too many attached devices<a class="hash-link" aria-label="Direct link to Too many attached devices" title="Direct link to Too many attached devices" href="/unraid-os/manual/storage-management/#too-many-attached-devices">​</a></h3>
<p><img decoding="async" loading="lazy" alt="indication that you have too many storage devices attached" src="/assets/images/Toomanydevices-aa282a55da5b4cdb1c106c1971e3bc3a.png" width="1998" height="170" class="img_ev3q"></p>
<p>Storage devices are any
devices that present themselves as a block storage device EXCLUDING the
USB flash device used to boot Unraid Server OS. Storage devices can be
attached via any of the following storage protocols:
IDE/SATA/SAS/SCSI/USB. This rule only applies prior to starting the
array. Once the array is started, you are free to attach additional
storage devices and make use of them (such as USB flash devices for
assignment to virtual machines). In Unraid Server OS 6, the attached
storage device limits are as follows:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="invalid-or-missing-key">Invalid or missing key<a class="hash-link" aria-label="Direct link to Invalid or missing key" title="Direct link to Invalid or missing key" href="/unraid-os/manual/storage-management/#invalid-or-missing-key">​</a></h3>
<p><img decoding="async" loading="lazy" alt="indication that your key is missing or invalid" src="/assets/images/Invalidormissingkey-dc584cc53dbbe567f6327956e0d2fa48.png" width="1558" height="184" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="missing-key">Missing key<a class="hash-link" aria-label="Direct link to Missing key" title="Direct link to Missing key" href="/unraid-os/manual/storage-management/#missing-key">​</a></h4>
<p>A valid registration key is required in order to start the array. To
purchase or get a trial key, perform the following steps:</p>
<ol>
<li>Log into the Unraid webGui using a browser (e.g. <a href="http://tower" target="_blank" rel="noopener noreferrer">http://tower</a> from
most device, <a href="http://tower.local" target="_blank" rel="noopener noreferrer">http://tower.local</a> from Mac devices)</li>
<li>Click on <strong>Tools</strong></li>
<li>Click on <strong>Registration</strong></li>
<li>Click to <strong>Purchase Key</strong> or <strong>Get Trial Key</strong> and complete the
steps presented there</li>
<li>Once you have your key file link, return to the <strong>Registration</strong> and
paste it in the field then click <strong>Install Key</strong>.</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="expired-trial">Expired trial<a class="hash-link" aria-label="Direct link to Expired trial" title="Direct link to Expired trial" href="/unraid-os/manual/storage-management/#expired-trial">​</a></h4>
<p>If the word &quot;expired&quot; is visible at the top left of the WebGUI, this
means your trial key has expired. Visit the registration page to request
either an extension to your trial or purchase a valid registration key.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="blacklisted-usb-flash-device">Blacklisted USB flash device<a class="hash-link" aria-label="Direct link to Blacklisted USB flash device" title="Direct link to Blacklisted USB flash device" href="/unraid-os/manual/storage-management/#blacklisted-usb-flash-device">​</a></h4>
<p>If your server is connected to the Internet and your trial hasn&#x27;t
expired yet, it is also possible that your USB flash device contains a
GUID that is prohibited from registering for a key. This could be
because the GUID is not truly unique to your device or has already been
registered by another user. It could also be because you are using an SD
card reader through a USB interface, which also tends to be provisioned
with a generic GUID. If a USB flash device is listed as blacklisted,
this is a permanent state and you will need to seek an alternative
device to use for your Unraid Server OS installation.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cannot-contact-key-server">Cannot contact key-server<a class="hash-link" aria-label="Direct link to Cannot contact key-server" title="Direct link to Cannot contact key-server" href="/unraid-os/manual/storage-management/#cannot-contact-key-server">​</a></h3>
<p>This message will only occur if you are using a Trial license. If you
are using a paid-for license then the array can be started without the
need to contact the Unraid license server.</p>
<p>If your server is unable to contact our key server to validate your
Trial license, you will not be able to start the array. The server will
attempt to validate upon first boot with a timeout of 30 sec. If it
can&#x27;t validate upon first boot, then the array won&#x27;t start, but each
time you navigate or refresh the WebGUI it will attempt validation again
(with a very short timeout). Once validated, it won&#x27;t phone-home for
validation again unless rebooted.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="this-unraid-server-os-release-has-been-withdrawn">This Unraid Server OS release has been withdrawn<a class="hash-link" aria-label="Direct link to This Unraid Server OS release has been withdrawn" title="Direct link to This Unraid Server OS release has been withdrawn" href="/unraid-os/manual/storage-management/#this-unraid-server-os-release-has-been-withdrawn">​</a></h3>
<p>If you receive this message, it means you are running a beta or release
candidate version of Unraid that has been marked disabled from active
use. Upgrade the OS to the latest stable, beta, or release candidate
version in order to start your array.</p>
<h1>Array operations</h1>
<p>There are a number of operations you can perform against your array:</p>
<ul>
<li>Add disks</li>
<li>Replace disks</li>
<li>Remove disks</li>
<li>Check disks</li>
<li>Spin disks up/down</li>
<li>Reset the array configuration</li>
</ul>
<p><em>NOTE: In cases where devices are added/replaced/removed, etc., the
instructions say &quot;Power down&quot; ... &quot;Power up&quot;. If your server&#x27;s
hardware is designed for hot/warm plug, Power cycling is not necessary
and Unraid is designed specifically to handle this. All servers built by
LimeTech since the beginning are like this:</em> <strong>no power cycle
necessary.</strong></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="adding-disks">Adding disks<a class="hash-link" aria-label="Direct link to Adding disks" title="Direct link to Adding disks" href="/unraid-os/manual/storage-management/#adding-disks">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="configuring-disks">Configuring Disks<a class="hash-link" aria-label="Direct link to Configuring Disks" title="Direct link to Configuring Disks" href="/unraid-os/manual/storage-management/#configuring-disks">​</a></h3>
<p>TBD</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="clear-v-pre-clear">Clear v Pre-Clear<a class="hash-link" aria-label="Direct link to Clear v Pre-Clear" title="Direct link to Clear v Pre-Clear" href="/unraid-os/manual/storage-management/#clear-v-pre-clear">​</a></h3>
<p>Under Unraid a &#x27;Clear disk is one that has been completely filled with
zeroes and contains a special signature to say that it is in this state.
This state is needed before a drive can be added to a parity-protected
array without affecting parity. If Unraid is in the process of writing
zeroes to all of a drive then this is referred to as a &#x27;Clear&#x27;
operation. This Clear operation can take place as a background operation
while using the array, but the drive in question cannot be used to store
data until the Clear operation has completed and the drive been
formatted to the desired File System type.</p>
<p><em>A disk that is being added as a parity drive or one that is to be used
to rebuild a failed drive does <strong>not</strong> need to be in a &#x27;Clear&#x27; state
as those processes overwrites every sector on the drive with new
contents as part of carrying out the operation. In addition, if you are
adding an additional data drive to an array that does not currently have
a parity drive there is no requirement for the drive to be clear before
adding it.</em></p>
<p>You will often see references in the forum or various wiki pages to
&#x27;Preclear&#x27;. This refers to getting the disk into a &#x27;Clear&#x27; state
before adding it to the array. The Preclear process requires the use of
a third-party plugin. Prior to Unraid v6, this was highly desirable as
the array was offline while Unraid carried out the &#x27;Clear&#x27; operation.
but Unraid v6 now carries out &#x27;Clear&#x27; as a background process with the
array operational while it is running so it is now completely optional.
Many users still like to use the Preclear process as in addition to
putting the disk into a clear state it also performs a level of &#x27;stress
test&#x27; on the drive which can be used as a confidence check on the
health of the drive. The Preclear as a result takes <strong>much</strong> longer than
Unraid&#x27;s more simplistic &#x27;clear&#x27; operation. Many users like to
Preclear new disks as an initial confidence check and to reduce the
chance of a drive suffering from &#x27;what is known as infant mortality&#x27;
where one of the most likely times for a drive to fail is when it is
first used (presumably due to a manufacturing defect).</p>
<p><em>It is also important to note that after completing a &#x27;Preclear&#x27; you
must <strong>not</strong> carry out any operation that will write to the drive (e.g.
<strong>format</strong> it) as this will destroy the &#x27;Clear&#x27; state.</em></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="data-disks">Data Disks<a class="hash-link" aria-label="Direct link to Data Disks" title="Direct link to Data Disks" href="/unraid-os/manual/storage-management/#data-disks">​</a></h3>
<p>This is the normal case of expanding the capacity of the system by
adding one or more new hard drives.</p>
<p>The capacity of any new disk(s) added must be the same size or smaller
than your parity disk. If you wish to add a new disk that is larger than
your parity disk, then you must instead first replace your parity disk.
(You could use your new disk to replace parity, and then use your old
parity disk as a new data disk).</p>
<p>The procedure is:</p>
<ol>
<li>Stop the array.</li>
<li>Power down the server.</li>
<li>Install your new disk(s).</li>
<li>Power up the server.</li>
<li>Assign the new storage device(s) to a disk slot(s) using the Unraid
WebGUI.</li>
<li>Start the array.</li>
<li>If your array is parity protected then Unraid will now automatically
begin to clear the disk as this is required before it can be added
to the array.<!-- -->
<ul>
<li>This step is omitted if you do not have a parity drive.</li>
<li>If a disk has been pre-cleared before adding it Unraid will
recognize this and go straight to the next step.</li>
<li>The clearing phase is necessary to preserve the fault tolerance
characteristic of the array. If at any time while the new
disk(s) is being cleared, one of the other disks fails, you will
still be able to recover the data of the failed disk.</li>
<li>The clearing phase can take several hours depending on the size
of the new disks(s) and although the array is available during
this process Unraid will not be able to use the new disk(s) for
storing files until the clear has completed and the new disk has
been formatted.</li>
<li>The files on other drives in the array will be accessible during
a clear operation, and the clear operation should not degrade
performance in accessing these other drives.</li>
</ul>
</li>
<li>Once the disk has been cleared, an option to format the disk will
appear in the WebGUI. At this point, the disk is added to the array
and shows as unmountable and the option to format unmountable disks
is shown.<!-- -->
<ul>
<li>Check that the serial number of the disk(s) is what you expect.
You do not want to format a different disk (thus erasing its
contents) by accident.</li>
</ul>
</li>
<li>Click the check box to confirm that you want to proceed with the
format procedure.<!-- -->
<ul>
<li>A warning dialog will be given warning you of the consequences
as once you start the format the disks listed will have any
existing contents erased and there is no going back. This
warning may seem a bit like over-kill but there have been times
that users have used the format option when it was <strong>not</strong> the
appropriate action.</li>
</ul>
</li>
<li>The format button will now be enabled so you can click on it to
start the formatting process.</li>
<li>The format should only take a few minutes and after the format
completes the disk will show as mounted and ready for use.<!-- -->
<ul>
<li>You will see that a small amount of space will already show as
used which is due to the overheads of creating the empty file
system on the drive.</li>
</ul>
</li>
</ol>
<p>You can add as many new disks to the array as you desire at one time,
but none of them will be available for use until they are all cleared
and formatted with a filesystem</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="parity-disks">Parity Disks<a class="hash-link" aria-label="Direct link to Parity Disks" title="Direct link to Parity Disks" href="/unraid-os/manual/storage-management/#parity-disks">​</a></h3>
<p>It is not mandatory for an Unraid system to have a parity disk, but it
is normal to provide redundancy. A parity disk can be added at any time,
Each parity disk provides redundancy against one data drive failing.</p>
<p>Any parity disk you add must be at least as large as the largest data
drive (although it can be larger). If you have two parity drives then it
is not required that they be the same size although it is required that
they both follow the rule of being at least as large as the largest data
drive.</p>
<p>The process for adding a parity disk is identical to that for adding a
data disk except that when you start the array after adding it Unraid
will start to build parity on the drive that you have just added.</p>
<p>While parity is being rebuilt the array will continue to function with
all existing files being available, but the performance in accessing
these files will normally be degraded due to contention with the parity
build process.</p>
<p><strong>NOTE:</strong></p>
<p>You cannot add a parity disk(s) and data disk(s) at the same time in a
single operation. This needs to be split into two separate steps, one to
add parity and the other to add additional data space.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="upgrading-parity-disks">Upgrading parity disk(s)<a class="hash-link" aria-label="Direct link to Upgrading parity disk(s)" title="Direct link to Upgrading parity disk(s)" href="/unraid-os/manual/storage-management/#upgrading-parity-disks">​</a></h3>
<p>You may wish to upgrade your parity device(s) to a larger one(s) so you
can start using larger sized disks in the array or to add an additional
parity drive</p>
<p><strong>CAUTION:</strong> If you take the actions below and only have a <strong>single</strong>
parity drive then you need to bear the following in mind:</p>
<ul>
<li>The array will be unprotected until the parity rebuild occurs. This
means that if a data drive fails during this process you are likely
to suffer loss of the data on the failing drive.</li>
<li>If you already have a failed data drive then this will remove the
ability to rebuild that data drive. In such a situation the <strong>Parity
Swap</strong> procedure is the correct way to proceed.</li>
</ul>
<p>The procedure to remove a parity drive is as follows:</p>
<ol>
<li>Stop the array.</li>
<li>Power down the server.</li>
<li>Install new larger parity disks. Note if you do this as your first
step then steps 2 &amp; 4 listed here are not needed.</li>
<li>Power up the server.</li>
<li>Assign the larger disk to the parity slot (replacing the former
parity device).</li>
<li>Start the array.</li>
</ol>
<p>When you start the array, the system will once again perform a parity
build to the new parity device and when it completes the array will once
again be in a protected state. It is recommended that you keep the old
parity drives contents intact until the above procedure completes as if
an array drive fails during this procedure so you cannot complete
building the contents of the new parity disk, then it is possible to use
the old one for recovery purposes (ask on the forum for the steps
involved). If you have a dual parity system and wish to upgrade both of
your parity disks, it is recommended to perform this procedure one
parity disk at a time, as this will allow for your array to still be in
a protected state throughout the entire upgrade process.</p>
<p>Once you&#x27;ve completed the upgrade process for a parity disk, the former
parity disk can be considered for assignment and use in the array as an
additional data disk (depending on age and durability)</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="replacing-disks">Replacing disks<a class="hash-link" aria-label="Direct link to Replacing disks" title="Direct link to Replacing disks" href="/unraid-os/manual/storage-management/#replacing-disks">​</a></h2>
<p>There are two primary reasons why you may wish to replace disks in the
array:</p>
<ul>
<li>A disk needs to be replaced due to failure or scheduled retirement
(out of warranty / support / serviceability).</li>
<li>The array is nearly full and you wish to replace existing data
disk(s) with larger ones (out of capacity).</li>
</ul>
<p>In either of these cases, the procedure to replace a disk is roughly the
same, but one should be aware of the risk of data loss during a disk
replacement activity. Parity device(s) protect the array from data loss
in the event a disk failure. A single parity device protects against a
single failure, whereas two parity devices can protect against losing
data when two disks in the array fail. This chart will help you better
understand your level of protection when various disk replacement
scenarios occur.</p>
<p>Data Protection During Disk Replacements</p>
<hr>
<p><strong>Replacing a single disk</strong>
<strong>Replacing two disks</strong></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="replacing-a-disk-to-increase-capacity">Replacing a disk to increase capacity<a class="hash-link" aria-label="Direct link to Replacing a disk to increase capacity" title="Direct link to Replacing a disk to increase capacity" href="/unraid-os/manual/storage-management/#replacing-a-disk-to-increase-capacity">​</a></h3>
<p>With modern disks rapidly increasing in capacity you can replace an
existing data drive with a larger one to increase the available space in
the array without increasing the total count of drives in the array.</p>
<p>Points to note are:</p>
<ul>
<li>If a disk is showing as unmountable then you should resolve this
before attempting to upgrade the drive as the rebuild process does
not clear an unmountable status</li>
<li>If you have single parity then you are not protected against a
different drive failing during the upgrade process. If this happens
then post to the forums to get advice on the best way to proceed to
avoid data loss.</li>
<li>If you have dual parity and you are upgrading a single data drive
then you are still protected against another data drive failing
during the upgrade process.</li>
<li>If you have dual parity you can upgrade two drives simultaneously
but you would then not be protected against another drive failing
while doing the upgrade. If this happens then post to the forums to
get advice on the best way to proceed to avoid data loss. It is up
to you to decide on whether to take the route of upgrading two
drives one at a time or taking the faster but riskier route of doing
them at the same time.</li>
<li>Keep the disk that you are replacing with its contents unchanged
until you are happy that the upgrade process has gone as planned.
This gives a fallback capability if the upgrade has gone wrong for
any reason.</li>
</ul>
<p>To perform the upgrade proceed as follows:</p>
<ul>
<li>Run a parity check if you have not done so recently and make sure
that zero errors are reported. Attempting an upgrade if parity is
not valid will result in the file system on the upgraded disk being
corrupt.</li>
<li>Stop the array.</li>
<li>Unassign the disk you want to upgrade.</li>
<li>Start the array to commit this change and make Unraid &#x27;forget&#x27; the
current assignment.<!-- -->
<ul>
<li>Unraid will now tell you that the missing disk is being
emulated. It does this using the combination of the remaining
data drives and a parity drive to dynamically reconstruct the
contents of the emulated drive. From a user perspective the
system will act as if the drive was still present albeit with a
reduced level of protection against another drive failing.</li>
<li>If you started the array in Maintenance mode then this will
ensure no new files can be written to the drive during the
upgrade process</li>
<li>If you started the drive in Normal mode then you will be able to
read and write to the Emulated drive as if it was still physically
present</li>
</ul>
</li>
<li>Stop the array.<!-- -->
<ul>
<li>At this point the array is in the same state as it would be if
the drive you have stopped using had failed instead of being
unassigned as part of the upgrade process.</li>
</ul>
</li>
<li>Assign the (larger) replacement drive to the slot previously used
for the drive you are upgrading.</li>
<li>Start the array to begin rebuilding the contents of the emulated
drive on to the upgraded drive.<!-- -->
<ul>
<li>Since the replacement drive is larger than the one it is
replacing when the contents of the emulated drive have been put
onto the replacement drive Unraid will automatically expand the
file system on the drive so the full capacity of the drive
becomes available for storing data.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="replacing-faileddisabled-disks">Replacing failed/disabled disk(s)<a class="hash-link" aria-label="Direct link to Replacing failed/disabled disk(s)" title="Direct link to Replacing failed/disabled disk(s)" href="/unraid-os/manual/storage-management/#replacing-faileddisabled-disks">​</a></h3>
<p><img decoding="async" loading="lazy" alt="a red X indicates that a disk has suffered a write error and should be replaced" src="/assets/images/Diskfailureindicator-b9a07c9faf62f81c27e878b8f9028ae1.png" width="1010" height="146" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" alt="if notifications are enabled, this additional alert will appear" src="/assets/images/Diskfailurenotification-fed01ecb6aed8a37b551176530abc4bd.png" width="756" height="140" class="img_ev3q"></p>
<p>As noted previously, with a single parity disk, you can replace up to
one disk at a time, but during the replacement process, you are at risk
for data loss should an additional disk failure occur. With two parity
disks, you can replace either one or two disks at a time, but during a
two disk replacement process, you are also at risk for data loss.
Another way to visualize the previous chart:</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="array-tolerance-to-disk-failure-events">Array Tolerance to Disk Failure Events<a class="hash-link" aria-label="Direct link to Array Tolerance to Disk Failure Events" title="Direct link to Array Tolerance to Disk Failure Events" href="/unraid-os/manual/storage-management/#array-tolerance-to-disk-failure-events">​</a></h4>
<table><thead><tr><th></th><th>Without Parity</th><th>With Single Parity</th><th>With Dual Parity</th></tr></thead><tbody><tr><td><strong>A single disk failure</strong></td><td>Data from that disk is lost</td><td>Data is still available and the disk can be replaced</td><td>Data is still available and the disk can be replaced</td></tr><tr><td><strong>A dual disk failure</strong></td><td>Data on both disks are lost</td><td>Data on both disks are lost</td><td>Data is still available and the disks can be replaced</td></tr></tbody></table>
<p><img decoding="async" loading="lazy" alt="confirming you wish to start the array and rebuild the contents of the failed disk on a new disk" src="/assets/images/Confirmrebuild-6cad3baf0748331ceb97a74344ace87f.png" width="1812" height="208" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" alt="notification indicating that a disk rebuild is occurring" src="/assets/images/Diskrebuildnotification-430933c6d14fcdd5e280ac3612657786.png" width="724" height="300" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" alt="the progress and time remaining for the rebuild will be displayed under the array operation section" src="/assets/images/Timeremaining-b5cabc06bc085ed1e797f7a4c554906d.png" width="1086" height="718" class="img_ev3q"></p>
<p><em>NOTE: If more disk failures have occurred than your parity protection
can allow for, you are advised to post in the General Support forum for
assistance with data recovery on the data devices that have failed.</em></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-failed-disabled-drive">What is a &#x27;failed&#x27; (disabled) drive<a class="hash-link" aria-label="Direct link to What is a &#x27;failed&#x27; (disabled) drive" title="Direct link to What is a &#x27;failed&#x27; (disabled) drive" href="/unraid-os/manual/storage-management/#what-is-a-failed-disabled-drive">​</a></h4>
<p>It is important to realize what is meant by the term failed drive:</p>
<ul>
<li>It is typically used to refer to a drive that is marked with a red
&#x27;x&#x27; in the Unraid GUI.</li>
<li>It does NOT necessarily mean that there is a physical problem with
the drive (although that is always a possibility). More often than
not the drive is OK and an external factor caused the write to fail.</li>
</ul>
<p>If the syslog shows that resets are occurring on the drive then this
is a good indication of a connection problem.</p>
<p>The SMART report for the drive is a good place to start.</p>
<p>The SMART attributes can indicate a drive is healthy when in fact it
is not. A better indication of health is whether the drive can
successfully complete the SMART extended test without error. If it
cannot complete this test error-free then there is a high likelihood
that the drive is not healthy.</p>
<p>CRC errors are almost invariably cabling issues. It is important to
realize that this SMART attribute is never reset to 0 so if it stops
increasing that is what you should be aiming to achieve.</p>
<ul>
<li>If you have sufficient parity drives then Unraid will emulate the
failed drive using the combination of the parity drive(s) and the
remaining &#x27;good&#x27; drives. <strong>From a user perspective, this results
in the system reacting as if the failed drive is still present</strong>.</li>
</ul>
<p>This is one reason why it is important that you have enabled
notifications to get alerted to such a failure. From the end-user
perspective, the system continues to operate and the data remain
available. Without notifications enabled the user may blithely
continue using their Unraid server not realizing that their data may
now be at risk and they need to take some corrective action.</p>
<p>When a disk is marked as disabled and Unraid indicates it is being
emulated then the following points apply:</p>
<ul>
<li><strong>Unraid will stop writing to the physical drive</strong>. Any writes to
the &#x27;emulated&#x27; drive will not be reflected on the physical drive
but will be reflected in parity so from the end-user perspective
then the array seems to be updating data as normal.</li>
<li>When you rebuild a disabled drive the process will make the physical
drive correspond to the emulated drive by doing a
<em>sector-for-sector</em> copy from the emulated drive to the physical
drive. You can, therefore, check that the emulated drive contains
the content that you expect before starting the rebuild process.</li>
<li><strong>If a drive is being emulated then you can carry out recovery
actions on the emulated drive before starting the rebuild process</strong>.
This can be important as it keeps the physical drive untouched for
potential data recovery processes if the emulated drive cannot be
recovered.</li>
<li>If an <strong>emulated</strong> drive is marked as <strong>unmountable</strong> then a rebuild
will <strong>not</strong> fix this and the rebuilt drive will have the same
unmountable status as the emulated drive. The correct handling of
unmountable drives is described in a later section. It is
recommended that you repair the file system before attempting a
rebuild as the repair process is much faster that the rebuild
process and if the repair process is not successful the rebuilt
drive would have the same problem.</li>
</ul>
<p>A replacement drive does not need to be the <em>same</em> size as the disk it
is replacing. It cannot be smaller but it can be larger. If the
replacement drive is not larger than any of your parity drives then the
simpler procedure below can be used. In the special case where you want
to use a new disk that is larger than at least one of your parity drives
then please refer to the <strong>Parity Swap</strong> procedure that follows instead.</p>
<p>If you have purchased a replacement drive, many users like to pre-clear
the drive to stress test the drive first, to make sure it&#x27;s a good
drive that won&#x27;t fail for a few years at least. The Preclearing is not
strictly necessary as replacement drives don&#x27;t have to be cleared since
they are going to be completely overwritten., but Preclearing new drives
one to three times provides a thorough test of the drive, eliminates
&#x27;infant mortality&#x27; failures. You can also carry out stress tests in
other ways such as running an extended SMART test or using tools
supplied by the disk manufacturer that run on Windows or macOS.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="normal-replacement">Normal replacement<a class="hash-link" aria-label="Direct link to Normal replacement" title="Direct link to Normal replacement" href="/unraid-os/manual/storage-management/#normal-replacement">​</a></h4>
<p>This is a normal case of replacing a failed drive where the replacement
drive is <strong>not</strong> larger than your current parity drive(s).</p>
<p>It is worth emphasizing that Unraid must be able to reliably read every
bit of parity PLUS every bit of ALL other disks in order to reliably
rebuild a missing or disabled disk. This is one reason why you want to
fix any disk-related issues with your Unraid server as soon as possible.</p>
<p>To replace a failed disk or disks:</p>
<ol>
<li>Stop the array.</li>
<li>Power down the unit.</li>
<li>Replace the failed disk(s) with a new one(s).</li>
<li>Power up the unit.</li>
<li>Assign the replacement disk(s) using the Unraid WebGUI.</li>
<li>Click the checkbox that says <em>Yes I want to do this</em></li>
<li>(optional) Tick the box to start in Maintenance mode. If you start
the array in Maintenance mode you will need to press the <strong>Sync</strong>
button to trigger the rebuild. The advantage of doing this in
Maintenance mode is that nothing else can write to the array while
the rebuild is running which maximises speed. The disadvantage is
that you cannot use the array in the meantime and until you return
to normal mode cannot see what the contents of the disk being
rebuilt will look like.</li>
<li>Click <strong>Start</strong> to initiate the rebuild process.and the system will
reconstruct the contents of the emulated disk(s) onto the new
disk(s) and, if the new disk(s) is/are bigger, expand the file
system.</li>
</ol>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="notes">Notes<a class="hash-link" aria-label="Direct link to Notes" title="Direct link to Notes" href="/unraid-os/manual/storage-management/#notes">​</a></h5>
<ul>
<li><strong>IMPORTANT</strong>: If at any point during the replacement process Unraid
appears to offer an option to format a drive do <strong>not</strong> do so as
this will result in wiping all files belonging to the drive you are
trying to replace and updating parity to reflect this.</li>
<li><strong>A &#x27;good&#x27; rebuild relies on all the other array disks being
read without error.</strong> If during the rebuild process any of the other
array disks start showing read errors then the rebuilt disk is going
to show corruption (and probably end up as unmountable) with some
data loss highly likely.</li>
<li>You must replace a failed disk with a disk that is as big or bigger
than the original and not bigger than the smallest parity disk.</li>
<li>If the replacement disk has been used before then remove any
existing partitions. In theory this should not be necessary but is
has been known to sometimes cause problems so it is better to play
safe.</li>
<li>The rebuild process can never be used to change the format of a
disk - it can only rebuild to the existing format.</li>
<li>The rebuild process will not correct a disk that is showing as
<em>unmountable</em> when being emulated (as this indicates there is some
level of file system corruption present) - it will still show as
<em>unmountable</em> after the rebuild as the rebuild process simply makes
the physical disk match the emulated one.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="rebuilding-a-drive-onto-itself">Rebuilding a drive onto itself<a class="hash-link" aria-label="Direct link to Rebuilding a drive onto itself" title="Direct link to Rebuilding a drive onto itself" href="/unraid-os/manual/storage-management/#rebuilding-a-drive-onto-itself">​</a></h4>
<p>There can be cases where it is determined that the reason a disk was
disabled is due to an external factor and the disk drive appears to be
fine. In such a case you need to take a slightly modified process to
cause Unraid to rebuild a &#x27;disabled&#x27; drive back onto the same drive.</p>
<ol>
<li>Stop array</li>
<li>Unassign disabled disk</li>
<li>Start array so the missing disk is registered</li>
<li><strong>Important</strong>: If the drive to be rebuilt is a data drive then check
that the emulated drive is showing the content you expect to be
there as the rebuild process simply makes the physical drive match
the emulated one. If this is not the case then you may want to ask
in forums for advice on the best way to proceed.</li>
<li>Stop array</li>
<li>Reassign disabled disk</li>
<li>(optional) Tick the box to start in Maintenance mode. If you start
the array in Maintenance mode you will need to press the <strong>Sync</strong>
button to trigger the rebuild. The advantage of doing this in
Maintenance mode is that nothing else can write to the array while
the rebuild is running which maximises speed. The disadvantage is
that you cannot use the array in the meantime and until you return
to normal mode cannot see what the contents of the disk being
rebuilt will look like.</li>
<li>Click <strong>Start</strong> to initiate the rebuild process and the system will
reconstruct the contents of the emulated disk.</li>
</ol>
<p>This process can be used for both data and parity drives that have been
disabled.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="parity-swap">Parity Swap<a class="hash-link" aria-label="Direct link to Parity Swap" title="Direct link to Parity Swap" href="/unraid-os/manual/storage-management/#parity-swap">​</a></h4>
<p>This is a special case of replacing a disabled drive where the
replacement drive is larger than your current parity drive. This
procedure applies to both the parity1 and the parity2 drives. If you
have dual parity then it can be used on both simultaneously to replace 2
disabled data drives with the 2 old parity drives.</p>
<p><strong>NOTE</strong>: It is <strong>not</strong> recommended that you use this procedure for
upgrading the size of both a parity drive and a data drive as the array
will be offline during the parity copy part of the operation. In such a
case it is normally better to first upgrade the parity drive and then
afterward upgrade the data drive using the drive replacement procedure.
This takes longer but the array remains available for use throughout the
process, and in addition, if anything goes wrong you have the just
removed drive available intact for recovery purposes</p>
<p><strong>Why would you want to do this? To replace a data drive with a larger
one, that is even larger than the Parity drive.</strong></p>
<p>Unraid does not require a replacement drive to be the same size as
the drive being replaced. The replacement drive CANNOT be smaller
than the old drive, but it CAN be larger, much larger in fact. If
the replacement drive is the same size or larger, UP TO the same
size as the smallest parity drive, then there the simple procedure
above can be used. If the replacement drive is LARGER than the
Parity drive, then a special two-step procedure is required as
described here. It works in two phases: - The larger-than-existing-parity drive is first upgraded to
become the new the parity drive - The old parity drive replaces the old data drive and the data of
the failed drive is rebuilt onto it.</p>
<p>As an example, you have a 1TB data drive that you want to replace
(the reason does not matter). You have a 2TB parity drive. You buy a
4TB drive as a replacement. The &#x27;Parity Swap&#x27; procedure will copy
the parity info from the current 2TB parity drive to the 4TB drive,
zero the rest, make it the new parity drive, then use the old 2TB
parity drive to replace the 1TB data drive. Now you can do as you
wish with the removed 1TB drive.</p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="important-notes">Important Notes<a class="hash-link" aria-label="Direct link to Important Notes" title="Direct link to Important Notes" href="/unraid-os/manual/storage-management/#important-notes">​</a></h5>
<ul>
<li>If you have purchased a replacement drive, we always recommend many
users to pre-clear the drive to stress test the replacement drive
first, to make sure it&#x27;s a good drive that won&#x27;t fail for a few
years at least. The Preclearing is not strictly necessary, as
replacement drives don&#x27;t have to be cleared, they are going to be
completely overwritten. But Preclearing new drives one to three
times provides a thorough test of the drive, eliminates &#x27;infant
mortality&#x27; failures.</li>
<li><strong>If your replacement drive is the same size or smaller than your
current Parity drive, then you don&#x27;t need this procedure. Proceed
with the <a href="/legacy/FAQ/replacing-a-data-drive/">Replacing a Data Drive</a>
procedure.</strong></li>
<li>This procedure is strictly for replacing data drives in an Unraid
array. If all you want to do is replace your Parity drive with a
larger one, then you don&#x27;t need the Parity Swap procedure. Just
remove the old parity drive and add the new one, and start the
array. The process of building parity will immediately begin. (If
something goes wrong, you still have the old parity drive that you
can put back!)</li>
<li><strong>IMPORTANT!!!</strong> This procedure REQUIRES that the data drive being
replaced MUST be disabled first. If the drive failed (has a red
ball), then it is already &#x27;disabled&#x27;, but if the drive is OK but
you want to replace it anyway, then you have to force it to be
&#x27;failed&#x27;, by unassigning it and starting and stopping the array.
Unraid only forgets a drive when the array is started without the
drive, otherwise it still associates it with the slot (but
&#x27;Missing&#x27;). The array must be started once with the drive
unassigned or disabled. Yes, it may seem odd, but is required before
Unraid will recognize that you are trying to do a &#x27;Parity Swap&#x27;.
It needs to see a disabled data disk with forgotten ID, a new disk
assigned to its slot that used to be the parity disk, and a new disk
assigned to the parity slot.</li>
<li>Obviously, it&#x27;s very important to identify the drives for
assignment correctly! Have a list of the drive models that will be
taking part in this procedure, with the last 4 characters of their
serial numbers. If the drives are recent Toshiba models, then they
may all end in <strong>GS</strong> or <strong>S</strong>, so you will want to note the
preceding 4 characters instead.</li>
</ul>
<p>The steps to carry out this procedure are:</p>
<p>Note: these steps are the general steps needed. The steps you take
may differ depending on your situation. If the drive to be replaced
has failed and Unraid has disabled it, then you may not need steps 1
and 2, and possibly not steps 3 and 4. If you have already installed
the new replacement drive (perhaps because you have been Preclearing
it), then you would skip steps 5 through 8. Revise the steps as
needed.</p>
<ol>
<li>
<p>Stop the array <em>(if it&#x27;s started)</em></p>
</li>
<li>
<p>Unassign the old drive <em>(if it&#x27;s still assigned)</em>
<em>If the drive was a good drive and notifications are enabled, you
will get error notifications for a missing drive! This is normal.</em></p>
</li>
<li>
<p>Start the array (put a check in the <strong>Yes I want to do this</strong>
checkbox if it appears (older versions: <strong>Yes, I&#x27;m sure</strong>))
<em>Yes, you need to do this. Your data drive should be showing as
<strong>Not installed</strong>.</em></p>
</li>
<li>
<p>Stop the array again</p>
</li>
<li>
<p>Power down</p>
</li>
<li>
<p><em>[ Optional ]</em> Pull the old drive
<em>You may want to leave it installed, for Preclearing or testing or
reassignment.</em></p>
</li>
<li>
<p>Install the new drive (preclear STRONGLY suggested, but formatting
not needed)</p>
</li>
<li>
<p>Power on</p>
</li>
<li>
<p>Stop the array</p>
<p><em>*If you get an &quot;Array Stopping•Retry unmounting disk
share(s)...&quot; message, try disabling Docker and/or VM in Settings
and stopping the array again after rebooting.</em></p>
</li>
<li>
<p>Unassign the parity drive</p>
</li>
<li>
<p>Assign the new drive in the parity slot
<em>You may see more error notifications! This is normal.</em></p>
</li>
<li>
<p>Assign the old parity drive in the slot of the old data drive being
replaced
<em>You should now have blue drive status indicators for both the
parity drive and the drive being replaced.</em></p>
</li>
<li>
<p>Go to the <strong>Main</strong> → <strong>Array Operation</strong> section
<em>You should now have a <strong>Copy</strong> button, with a statement indicating
&quot;<strong>Copy</strong> will copy the parity information to the new parity
disk&quot;.</em></p>
</li>
<li>
<p>Put a check in the <strong>Yes I want to do this</strong> checkbox (older
versions: <strong>Yes, I&#x27;m sure</strong>), and click the <strong>Copy</strong> button
_Now patiently watch the copy progress, takes a long time (~20
hours for 4TB on a 3GHz Core 2 Duo). All of the contents of the old
parity drive are being copied onto the new drive, then the remainder
of the new parity drive will be zeroed.</p>
<p><strong>The array will NOT be available during this operation!</strong></p>
<p><em>*If you disabled Docker and/or VM in settings earlier, go ahead
and re-enable now.</em></p>
<p>When the copy completes, the array will still be stopped
(&quot;<strong>Stopped</strong>. Upgrading disk/swapping parity.&quot;).</p>
<p>The <strong>Start</strong> button will now be present, and the description will
now indicate that it is ready to start a Data-Rebuild._</p>
</li>
<li>
<p>Put a check in the <strong>Yes I want to do this</strong> checkbox (older
versions: <strong>Yes, I&#x27;m sure</strong>), and click the <strong>Start</strong> button
_The data drive rebuild begins. Parity is now valid, and the array
is started.</p>
<p>Because the array is started, you can use the array as normal, but
for best performance, we recommend you limit your usage.</p>
<p>Once again, you can patiently watch the progress, it takes a long
time too! All of the contents of the old data drive are now being
reconstructed on what used to be your parity drive, but is now
assigned as the replacement data drive._</p>
</li>
</ol>
<p><strong>That&#x27;s it!</strong> Once done, you have an array with a larger parity
drive and a replaced data drive that may also be larger!</p>
<p><em>Note: many users like to follow up with a parity check, just to
check everything. It&#x27;s a good confidence builder (although not
strictly necessary)!</em></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="a-disk-failed-while-i-was-rebuilding-another">A disk failed while I was rebuilding another<a class="hash-link" aria-label="Direct link to A disk failed while I was rebuilding another" title="Direct link to A disk failed while I was rebuilding another" href="/unraid-os/manual/storage-management/#a-disk-failed-while-i-was-rebuilding-another">​</a></h3>
<p>If you only have a single parity device in your system and a disk
failure occurs during a data-rebuild event, the data rebuild will be
cancelled as parity will no longer be valid. However, if you have dual
parity disks assigned in your array, you have options. You can either</p>
<ul>
<li>let the first disk rebuild complete before starting the second, or</li>
<li>you can cancel the first rebuild, stop the array, replace the second
failed disk, then start the array again</li>
</ul>
<p>If the first disk being rebuilt is nearly complete, it&#x27;s probably
better to let that finish, but if you only just began rebuilding the
first disk when the second disk failure occurred, you may decide
rebuilding both at the same time is a better solution.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="removing-disks">Removing disks<a class="hash-link" aria-label="Direct link to Removing disks" title="Direct link to Removing disks" href="/unraid-os/manual/storage-management/#removing-disks">​</a></h2>
<p>There may be times when you wish to remove drives from the system.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="removing-parity-disks">Removing parity disk(s)<a class="hash-link" aria-label="Direct link to Removing parity disk(s)" title="Direct link to Removing parity disk(s)" href="/unraid-os/manual/storage-management/#removing-parity-disks">​</a></h3>
<p>If for some reason you decide you do not need the level of parity
protection that you have in place then it is always possible to easily
remove a parity disk.</p>
<ol>
<li>Stop the array.</li>
<li>Set the slot for the parity disk you wish to remove to <em>Unassigned</em>.</li>
<li>Start the array to commit the change and &#x27;forget&#x27; the previously
assigned parity drive.</li>
</ol>
<p><strong>CAUTION:</strong> If you already have any failed data drives in the array be
aware that removing a parity drive reduces the number of failed drives
Unraid can handle without potential data loss.</p>
<ul>
<li>If you started with <strong>dual</strong> parity you can still handle a single
failed drive but would not then be able to sustain another drive
failing while trying to rebuild the already failed drive without
potential data loss.</li>
<li>If you started with <strong>single</strong> parity you will no longer be able to
handle any array drive failing without potential data loss.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="removing-data-disks">Removing data disk(s)<a class="hash-link" aria-label="Direct link to Removing data disk(s)" title="Direct link to Removing data disk(s)" href="/unraid-os/manual/storage-management/#removing-data-disks">​</a></h3>
<p>Removing a disk from the array is possible, but normally requires you to
once again sync your parity disk(s) after doing so. This means that
until the parity sync completes, the array is vulnerable to data loss
should any disk in the array fail.</p>
<p>To remove a disk from your array, perform the following steps:</p>
<ol>
<li>Stop the array</li>
<li>(optional) Make note if your disk assignments under the main tab
(for both the array and cache; some find it helpful to take a
screenshot)</li>
<li>Perform the <a href="/unraid-os/manual/storage-management/#reset-the-array-configuration">Reset the array
configuration</a>
procedure. When doing this it is a good idea to use the option to
preserve all current assignments to avoid you having to re-enter
them (and possibly make a mistake doing so).</li>
<li>Make sure all your previously assigned disks are there and set the
drive you want removed to be Unassigned</li>
<li>Start the array without checking the &#x27;Parity is valid&#x27; box.</li>
</ol>
<p>A parity-sync will occur if at least one parity disk is assigned and
until that operation completes, the array is vulnerable to data loss
should a disk failure occur.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="alternative-method">Alternative method<a class="hash-link" aria-label="Direct link to Alternative method" title="Direct link to Alternative method" href="/unraid-os/manual/storage-management/#alternative-method">​</a></h3>
<p>It is also possible to remove a disk without invalidating parity if
special action is taken to make sure that the disk only contains zeroes
as a disk that is all zeroes does not affect parity. There is no support
for this method built into the Unraid GUI so. it requires manual steps
to carry out the zeroing process. It also takes much longer than the
simpler procedure above.</p>
<p><em>There is no official support from Limetech for using this method so you
are doing it at your own risk.</em></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="notes-1">Notes<a class="hash-link" aria-label="Direct link to Notes" title="Direct link to Notes" href="/unraid-os/manual/storage-management/#notes-1">​</a></h3>
<ol>
<li>This method preserves parity protection at all times.</li>
<li>This method can only be used if the drive to be removed is a good
drive that is completely empty, is mounted and can be completely
cleared without errors occurring</li>
<li>This method is limited to removing only one drive at a time
(actually this is not technically true but trying to do multiple
drives in parallel is slower than doing them sequentially due to the
contention that arises for updating the parity drive)</li>
<li>As stated above, the drive must be completely empty as this process
will erase all existing content. If there are still any files on it
(including hidden ones), they must be moved to another drive or
deleted.<!-- -->
<ul>
<li>One quick way to clear a drive of files is to reformat it! To
format an array drive, you stop the array, and then on the Main
page click on the link for the drive and change the file system
type to something different than it currently is, then restart
the array. You will then be presented with an option to format
it. Formatting a drive removes all of its data, and the parity
drive is updated accordingly, so the data cannot be easily
recovered.</li>
<li>Explanatory note: &quot;Since you are going to clear the drive
anyway, why do I have to empty it? And what is the purpose of
this strange clear-me folder?&quot; Yes, it seems a bit draconian to
require the drive to be empty since we&#x27;re about to clear and
empty it in the script, but we&#x27;re trying to be absolutely
certain we don&#x27;t cause data loss. In the past, some users
misunderstood the procedure, and somehow thought we would
preserve their data while clearing the drive! This way, by
requiring the user to remove all data, and then add an odd
marker, there cannot be any accidents or misunderstandings and
data loss.</li>
</ul>
</li>
</ol>
<p>The procedure is as follows:</p>
<ol>
<li>Make sure that the drive you are removing has been removed from any
inclusions or exclusions for all shares, including in the global
share settings.</li>
<li>Make sure the array is started, with the drive assigned and mounted.</li>
<li>Make sure you have a copy of your array assignments, especially the
parity drive.<!-- -->
<ul>
<li>In theory you should not need this but it is a useful safety net
in case if the &quot;Retain current configuration&quot; option under New
Config doesn&#x27;t work correctly (or you make a mistake using it).</li>
</ul>
</li>
<li>It is highly recommended to turn on reconstruct write, as the write
method (sometimes called &#x27;Turbo write&#x27;). With it on, the script
can run 2 to 3 times as fast, saving hours!<!-- -->
<ul>
<li>However when using &#x27;Turbo Write&#x27; all drives must read without
error so do not use it unless you are sure no other drive is
having issues.</li>
<li>To enable &#x27;turbo Write&#x27; in <em>Settings → Disk Settings</em>, change
Tunable (md_write_method) to reconstruct write</li>
</ul>
</li>
<li>Make sure ALL data has been copied off the drive; drive MUST be
completely empty for the clearing script to work.</li>
<li>Double check that there are no files or folders left on the drive.<!-- -->
<ul>
<li>Note: one quick way to clean a drive is to reformat it! (once
you&#x27;re sure nothing of importance is left of course!)</li>
</ul>
</li>
<li>Create a single folder on the drive with the name <strong>clear-me</strong> -
exactly 7 lowercase letters and one hyphen</li>
<li>Run the <a href="https://forums.unraid.net/forum/index.php?topic=50416.msg494968#msg494968" target="_blank" rel="noopener noreferrer">clear an array
drive</a>
script from the <a href="https://forums.unraid.net/forum/index.php?topic=49992" target="_blank" rel="noopener noreferrer">User
Scripts</a>
plugin (or run it standalone, at a command prompt).<!-- -->
<ul>
<li>If you prepared the drive correctly, it will completely and
safely zero out the drive. If you didn&#x27;t prepare the drive
correctly, the script will refuse to run, in order to avoid any
chance of data loss.</li>
<li>If the script refuses to run, indicating it did not find a
marked and empty drive, then very likely there are still files
on your drive. Check for hidden files. ALL files must be
removed!</li>
<li>Clearing takes a loooong time! Progress info will be displayed.</li>
<li>For best performance, make sure there are no reads/writes
happening to the array. The easiest way to do this is to bring
the array up in maintenance mode.</li>
<li>If running in User Scripts, the browser tab will hang for the
entire clearing process.</li>
<li>While the script is running, the Main screen may show invalid
numbers for the drive, ignore them. Important! Do not try to
access the drive, at all!</li>
</ul>
</li>
<li>When the clearing is complete, stop the array</li>
<li>Follow the procedure for resetting the array making sure you elect
to retain all current assignments.</li>
<li>Return to the Main page, and check all assignments. If any are
missing, correct them. Unassign the drive(s) you are removing.
Double-check all of the assignments, especially the parity drive(s)!</li>
<li>Click the check box for Parity is already valid, make sure it is
checked!</li>
<li>Start the array! Click the Start button then the Proceed button (on
the warning popup that will pop up)</li>
<li>(Optional) Start a correcting parity check to ensure parity really
is valid and you did not make a mistake in the procedure. If
everything was done correctly this should return zero errors.</li>
</ol>
<p>Alternate Procedure steps for Linux proficient users</p>
<p>If you are happy to use the Linux Command line then you can replace
steps 7 and 8 by performing the clearing commands yourself at a command
prompt. (Clearing takes just as long though!) If you would rather do
that than run the script in steps 7 and 8, then here are the 2 commands
to perform:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token function" style="color:rgb(80, 250, 123)">umount</span><span class="token plain"> /mnt/diskX</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">dd</span><span class="token plain"> </span><span class="token assign-left variable" style="color:rgb(189, 147, 249);font-style:italic">bs</span><span class="token operator">=</span><span class="token plain">1M </span><span class="token assign-left variable" style="color:rgb(189, 147, 249);font-style:italic">if</span><span class="token operator">=</span><span class="token plain">/dev/zero </span><span class="token assign-left variable" style="color:rgb(189, 147, 249);font-style:italic">of</span><span class="token operator">=</span><span class="token plain">/dev/mdX </span><span class="token assign-left variable" style="color:rgb(189, 147, 249);font-style:italic">status</span><span class="token operator">=</span><span class="token plain">progress</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>(where X in both lines is the number of the data drive being removed)
<strong>Important!!!</strong> It is VITAL you use the correct drive number, or you
will wipe clean the wrong drive! That&#x27;s why using the script is
recommended, because it&#x27;s designed to protect you from accidentally
clearing the wrong drive.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="checking-array-devices">Checking array devices<a class="hash-link" aria-label="Direct link to Checking array devices" title="Direct link to Checking array devices" href="/unraid-os/manual/storage-management/#checking-array-devices">​</a></h2>
<p><img decoding="async" loading="lazy" alt="the check button lets you perform parity and read checks" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAZYAAABDCAYAAAC/fCO+AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAABATSURBVHhe7Z3djxNXmsb3T+ImN3szVhTfdEUCS1msCHnoWUdItZqVZ8RY2cYwq9qL+GLHy2q9ysq7A7MzkSckvWRkIDgi6ygrS4ADtEmDQ4hJIufLgcRw4Zs4RHr2fU99uPzVXy4JGz0/6VBd51Sdr2rep95zqs/5KxBCCCERQmEhhBASKRQWQgghkUJhIYQQEikUFkIIIZFCYSGEEBIpFBZCCCGRQmEhhBASKRQWQgghkUJhIYQQEikUFkIIIZFCYSGEEBIpFBZCCCGRQmEhhBASKRQWQgghkUJhIYQQEikUFkIIIZFCYSGEEBIpFBZCCCGRQmEhhBASKRQWQhaQfruKfCaBeCyGWCwOy86j2u67id0KbIm3K133PAKaBS2ngKZ3vme0bnYFpmbNAmIFzbGLim1jN9XtVmyYW3eFljO9DW77wiGOlFOB36W7Qtsleey+fpNs+ZxNeyTe788IiOw5bwOFhZBFo1NGSgxNPJnDeq2BRq2CfDqOWNwzzossLGHmEJZmIb4Hw91EIb6VsGRQbkh/aqg4SGgf52rYvbYM0O/35V+lj7oT39uz2O45U1gIIdEwMIYqFsuhFrZ4/TqcRApOTUyMLyylKkq2JW+7cSTFQAbGp9/04sVoWTaKjZ6XIEmtEjIJzV/SEhmUWm4hIwZH849PGstB3YFVbHlnHZSTKm5+3g3krSJau/RYujUHqWQSyWQCiZQDbV6vmoUV1/olkatK/v2WtEfS5bqE3wfmZi2riFJO2mP/GWeybl8kxFDrbWEmDar2cyiu10DR67NYPFSGb9wLVdRMOdK2wGPx0vQeDfZZnNM8tR+8u9EqwpI0p+7K0JAdPGe/bLuEasl2+0TaFlRNnkHVSQ3rXGkHIjnPc44CCgshC4W+dct//uwWb9KesMQsMcSdLlpidHQYxTVePdSyYlBSJbTlrC+GOx7PoqaGtldDVvJOFBro9ruo5y0xSK5hCwyOiFIhIcZo2ltyT8pNr7vxfckrk0Em3zBJxoA6dQx2JSwiRrG8/OvRrqJQ68gPrkH1PZZW0UJqXeMFNbzaHu0cYxiTCLTO9EtYPIbMFBbj4YjXkU+ICLvp3XXtzzGvQQxwplRDQ8RkMDIUJv0lP/vGWcU3Hkui7FW3U07KtQ4mdEXv2+45+2XHLBGaDroiFvrc49rPktoqJqRe8jugGbRL4v0k3L6Y9zlHAIWFkIXCNVSuQZ6BJyxJ33p558a4qfEPpw3EEMt5VqxKv5YVozI0emFcg5NDoSDGyhJjP9XaqZfiGclGXvJsopwpS6wWa5syTF12LCwtFK0EnGobvRHDOyoso4Ty0rLC3sG2wjI5FGZJIZM2PywcnnFPuu00bCEspr/FqLv9797rC8EoO3jOE2V756Z/PWEK8nY9SPUo53/O8zO3sDx+/Bjff/89ut0uvv3226UIWlets9adkMVCja1rPMZGc4aEhWT83PvZDM2EgqbphPjwTXwU1+D4Yfo1SlMMkr4Vd8pipMVwNfJyrahC3UljXe/R8ncsLEKvgbKTQUI8Assuwh21cw2ob3P77QqctA6XucEKzzWF37hN27cSlnDQyftqIBZmSE4NdTyBpDeENCIs4XK2EhZB54fM9erVBZ6kJyQmaB138Jwnyh4TliC/UJBKRfGc52UuYfnxxx9x//59Y6QfPnyIR48eLUXQumqdVWS0DYQsDv7Ye2Z0nmAghiRlI18XSxAWEiV8boxZDGlj5UfZ/k3WQWMgBk+HSORNeNrLrA71pNebYuDcISzNM9+Qe1Ke57BbYQmhxt0yQ0NhYVEDnERJx/UMXayn9yos09P8Pgvmj8QbU8O7V2Fx51WyqNYKiMf9YbAe2r631BAPbSfPeaLs8LknTP5QZIgonvO87FlY9G1fDXOv1zOGelmPKoz0XMhCoQZS3p6Dr4UaVRT0ayF/DD0sJP71wXkP1YwYDzH0TfEkevW8GLmUa2TGxt6bxVRg2MKGtyeGKS5lFZqTAzhmqE3nVvy5gU4ZGceB7VpZty47FZZODU4pZNjaJSRNvn3UsiHDHXwlJUZT8kzGp3hHihGIvQtL3Kmh222hbLseS86dyNmRsJh7+35/uUZfvavpw2Ae2z3nibJHz3XuKRYXAesOMOisy++AekdS5yie85zsWVj0zf/BgwfmzX+Zg7ZB20LIQtFtoBj++4aUg8qsv2MZPw99FWa+FqoOX131ayHbcg2nfi3kfzE2ang7KKfkPBWaVwhwx/KDORwzAe/O4Rh2IywiIPrhQSLhDXN5X4Up3WpG3vYTcGo9yTILy0pIurzJ1zpyTxJWIofqnTFhkZ+qmbi0S/IJewHClsJi6pE2/aVf0ZVabZNPLK7GeDth6cupCIjem6kGw1rG6AfDYFuw1XM2/RYue/w8/FWYBbuknyG47OU56xDaiOc1B3sWFn3TV6Os4bvvvlvao7ZDAyGEzM8Afd/rSYgBj94ZWAr2LCz+RLga5afhSAghc+N5M3Eri/U9/Vn/08HcwuJ/DbbsR0IIIdEwl7B8/fXX+Oabb5b+SGEhhJDomEtYvvrqq6ciUFgIISQ65hKWL774Al9++eWOjpsX/x1/f9CClUggYVlI/sMfcPmepm/iD397Au/uMJ+Zx3d/i30n3t359aEjhYUQQqJjLmHpdDo7Cu1L/wTr4L/gUmsYd/ONl3Hw5QtodW7i9C9O4FLo+j2FSydEWC5NT9smUFgIISQ65hKWzz//fAfhNs78Mo7fvTctTcMGTv3iOE796WUcOngAlnUI/1H30jbO4vihAzhwQMMR/Nt7d934xhm87Mf/8hTqdyXunePYd/wdL/0Ujhw5hUZQxtaBwkIIIdExl7B89tln+PTTT7c5VuE8s4o/Xp+Vfh2/X43DqXrnlRyecaoSv4k/HTmCP2568Xf+B7+O/w7/a+IP4tX/c+OvvPo3WP39DXxazYmwyH1X/oxfHzqO6p3xcmYfKSxkkRkMBrh69SrefvttnDt3bmrQtCtXrphrCXnSzCUs9+7d20F4F/8cX8V/X5uWpuEa/ms1h4p/flEEIndRfr6I3L6/xorxVvzwr7ho4nNyDOfh3ferHHIrh/Dq5bG0bQKFhSwyKhgqLLoM0VZBr7l8+bJ3FyFPjrmEpd1u45NPPtnm2ET5755B9i+3Z6R/gP9cPYYL/vmFY9h37G2Jv4Bj+47hLxPXu/HB9eH7Vn6Fk/94GAfk/tsT980+UljIInP+/PmpyxFpuH79Oo6IZ69HPddrCXnSzCUsH3/8Me7evbvtcfP8b7Gysoa3bgzjN95awwu/eQsbdxsoHV7DBf/6C2vYt3ZertvAa3Yca2/dcuM3TiPz0mnUJf70S/tx8j0vn9Mv4YViPXTfLZz5zcrwvin1GT9SWMgi8+abb5q/uRoP+mL04osv4rnnnsP+/ftx8+ZNc+1MmgUk/BV8BV2peGSjKX+zLu/UpYVSbrgG1mLTQzVXcldZXjJ61RxKy1jxGcwtLHfu3NnR8crZAuwXfoafyX+AlZUVvLR2CrVNTb9ihOWcf/05VyDMfVfPYO2F/eY/zf4Xj+K1updf/TUc9eN/XsA7mk/4vs3zWNv/c5y+Oix/qyOFhSwy5XLZzAeOB9u28eyzz5pw8uRJE6fXzkaXZB8uLNksZJHJDHc31N0OZy+aqCsOz1rE8QkjgpiNaPHEabSK2W2X/I8MXWnZXd1yqZlLWFqtFj766KOlP1JYyCLjC8vt27dx4sQJc3zllVcCUTl69GggNlsLi+4B4q8y3ELRLqNZywWr9Nay7pa/usqtUyjBTunbv4iRXcE98XZSsZQYvZoIUxe1vI2MU4CTGe6nHkZXJE6KaDmZFOyiu+quH1dwMkjn68YLGilLyrALReRS7p71Zt92O4d8PodM3t/Tv4tKNmmW6s+kbBSbd1CxLVh2HuvNe/KzJ379JorpNLL5PLLpLNY7KpjSlnQeRScLx0kjkXFXCe43i0jLNfl8FulMGa2wtnYqsC0Ldn4dTdM3k20YoqsPS3+WpN3Sz0ndusA0vAYnnZF2Sl2kzm5/6bWST8lGSlwV7YeCXNwspBBLZVGo1YZ73AjuPjiTa00vKnMJi/6CPw2BwkIWGRUL/T1dXV01QnLw4MFAVA4fPmzSdiYs+kKcdZe475TF4IvZ6q4jo0fdUtdbjl2NXDLYWcsVFnfHQtdoax6ZYHcqEaiQATToW3doCXnDWFyrmDJ7joyUpQs45vyhudF8e9VMsL3ysGwXzcNd7l2NtVtHP39Dr+rtHyNtCDbe8na/lNvUmJc8NRl02+i4FfDQPD0xntGGIXptSkTMPdM625XRump/p4OtBIYbmLnCIj9oH3geS2c97XmQ+kKQHd0QbMGZS1g2Nzdx69atpT9SWMgi43ssx48fDwRFw/PPP4/3338/EJWdCItYMNhOXV6ifU/F9Vza6pF4e6wERs4wKSyansoWUCj4Qb2YEFrGMAOXsTi/jJGy1GMJxpykPMtGPiijIB5Jf/R6D40bF5ZmIbz/S6gNnniaWP+afge1snpAKfGMpG/cZI+QsMxow5Bh+Qa/PeqxSB65vLRDxEz32R+/NsgrJCxGEFXIVPTDc2FLwFzC8uGHHz4VgcJCFhlfWMbF5ezZsyOiomFbYTGbO+lQzfDNvVkQkREj6799jxrMkFGOu4Zwmtcwwg48Fn/v/JGyRoRliickTCu7N0VYRrwJFQTfY5kQlgG6Td0q2KUrXsKw7UpvKCwz2jAkJEKC77FoG4MdhPUDCVPAbGGJBxVwhy6Lxdz2G4YtGHMJS7PZfCoChYUsMm+88caIeKi4+JP14+HMmTPeXbPRSXrdhdA3kDp+H/dEQ5kuLH3UcwmkMxUx9sM5lnxOjuX22JdkmkcWyXRWDHFojqWWRyaXd+8ptdy4mcIidtyfYymIN2HnUTcVFoOcTSKdFYExcyySS7uMlPxcqOt8i9eOfguljM5ruPe6O1NO91h0r313jiUHO1PC+JRRu5xCSvLV8qe1YYiKhYi0eD86X5Py5lgGzaL8LPmLmOcr63CSNkrh+SC90++Hfh25RBqZiqdYKkTWpMAuOnMJi37eeOPGDWxsbCztkcJCFp1Lly6ZME1IwsG/jjwpRr2QKBio2C7RpL3PnoVF/xhL5yj0D7OWOWgbtC2ELCq6GZ0Khnour7/++tSgf7+i1/Al6UkSpbD00SyL12OPfaW2JOxZWB4/fmw2yfrggw9w7dq1pT1qG3766SevVYQsHrr+18OHD/HgwYMtgy7rwrXCyCKwZ2FRfvjhB/MXwDqcpG//ulZRo9FY+KMKig7jqahoGwghhETHXMKiqOfy6NEj3L9/37jhyxC0rlpneiqEEBI9cwsLIYQQEobCQgghJFIoLIQQQiKFwkIIISRSKCyEEEIihcJCCCEkUigshBBCIoXCQgghJFIoLIQQQiKFwkIIISRSKCyEEEIihcJCCCEkUigshBBCIoXCQgghJFIoLIQQQiIE+H/g/4xodwObUAAAAABJRU5ErkJggg==" width="406" height="67" class="img_ev3q"></p>
<p>When the array is started,
there is a button under <strong>Array Operations</strong> labelled <em>Check</em>. Depending
on whether or not you have any parity devices assigned, one of two
operations will be performed when clicking this button.</p>
<p>It is also possible to schedule checks to be run automatically at
User-defined intervals under <strong>Settings → Scheduler</strong>. It is a good idea
to do this as an automated check on array health so that problems can be
noticed and fixed before the array can deteriorate beyond repair.
Typical periods for such automated checks are monthly or quarterly and
it is recommended that such checks should be <strong>non-correcting</strong>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="parity-check">Parity check<a class="hash-link" aria-label="Direct link to Parity check" title="Direct link to Parity check" href="/unraid-os/manual/storage-management/#parity-check">​</a></h3>
<p>If you have at least one parity device assigned, clicking <em>Check</em>
will initiate a Parity-check. This will march through all data disks in
parallel, computing parity and checking it against stored parity on the
parity disk(s).</p>
<p>You can continue to use the array while a parity check is running but
the performance of any file operations will be degraded due to drive
contention between between the check and the file operation. The parity
check will also be slowed while any such file operations are active.</p>
<p>By default, if an error is found during a Parity-check the parity disk
will be updated (written) with the computed data and the Sync Errors
counter will be incremented. If you wish to run purely a check without
writing correction, uncheck the checkbox that says <strong>Write corrections
to parity</strong> before starting the check. In this mode, parity errors will
be notated but not actually fixed during the check operation.</p>
<p>A <em>correcting</em> parity check is started automatically when starting the
array after an &quot;Unsafe Shutdown&quot;. An &quot;Unsafe Shutdown&quot; is defined as
any time that the Unraid server was restarted without having previously
successfully stopped the array. The most common cause of Sync Errors is
an unexpected power-loss, which prevents buffered write data from being
written to disk. It is highly recommended that users consider purchasing
a UPS (uninterruptable power supply) for their systems so that Unraid
can be set to shut down tidily on power loss, especially if frequent
offsite backups aren&#x27;t being performed.</p>
<p>It is also recommended that you run an automatic parity check
periodically and this can be done under <em>Settings-&gt;Scheduler</em>. The
frequency is up to the user but monthly or quarterly are typical
choices. It is also recommended that such a check is set as
<strong>non-correcting</strong> as if a disk is having problems there is a chance of
you corrupting your parity if you set such a check to be correcting. The
only acceptable result from such a check is to have <strong>0 errors</strong>
reported. If you do have errors reported then you should take
pre-emptive action to try and find out what is causing them. If in doubt
ask questions in the forum.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="read-check">Read check<a class="hash-link" aria-label="Direct link to Read check" title="Direct link to Read check" href="/unraid-os/manual/storage-management/#read-check">​</a></h3>
<p><img decoding="async" loading="lazy" alt="history lets you review stats on your preview check operations" src="data:image/png;base64,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" width="789" height="54" class="img_ev3q"></p>
<p>If you configure an array
without any parity devices assigned, the <em>Check</em> option will start a
<em>Read check</em> against all the devices in the array. You can use this to
check disks in the array for unrecoverable read errors, but know that
without a parity device, data may be lost if errors are detected.</p>
<p>A <em>Read Check</em> is also the type of check started if you have disabled
drives present and the number of disabled drives is larger than the
number of parity drives.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="check-history">Check history<a class="hash-link" aria-label="Direct link to Check history" title="Direct link to Check history" href="/unraid-os/manual/storage-management/#check-history">​</a></h3>
<p>Any time a parity or read check is performed, the system will log the
details of the operation and you can review them by clicking the
<em>History</em> button under <strong>Array Operations</strong>. These are stored in a text
file under the config directory on your Unraid USB flash device.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="spin-up-and-down-disks">Spin up and down disks<a class="hash-link" aria-label="Direct link to Spin up and down disks" title="Direct link to Spin up and down disks" href="/unraid-os/manual/storage-management/#spin-up-and-down-disks">​</a></h2>
<p>If you wish to manually control the spin state of your rotational
storage devices or toggle your SSD between active and standby mode,
these buttons provide that control. Know that if files are in the
process of being accessed while using these controls, the disk(s) in use
will remain in an active state, ignoring your request.</p>
<p>When disks are in a spun-down state, they will not report their
temperature through the WebGUI.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="reset-the-array-configuration">Reset the array configuration<a class="hash-link" aria-label="Direct link to Reset the array configuration" title="Direct link to Reset the array configuration" href="/unraid-os/manual/storage-management/#reset-the-array-configuration">​</a></h2>
<p><img decoding="async" loading="lazy" alt="you can reset your disk configuration from the new config page" src="/assets/images/Newconfig-e2b06a7cf5cec628f544d2b1982ca621.png" width="1106" height="309" class="img_ev3q"></p>
<p>If you wish to remove a disk from the array or you simply wish to start
from scratch to build your array configuration, there is a tool in
Unraid that will do this for you. To reset the array configuration,
perform the following steps:</p>
<ol>
<li>Navigate to the <strong>Tools</strong> page and click <em>New Config</em></li>
<li>You can (optionally) elect to have the system preserve some of the
current assignments while resetting the array. This can be very
useful if you only intend to make a small change as it avoids you
having to re-enter the details of the disks you want to leave
unchanged.</li>
<li>Click the checkbox confirming that you want to do this and then
click apply to perform the operation</li>
<li>Return to the <strong>Main</strong> tab and your configuration will have been
reset</li>
<li>Make any adjustments to the configuration that you want.</li>
<li>Start the array to commit the configuration. You can start in Normal
or Maintenance mode.</li>
</ol>
<p>Notes:</p>
<ul>
<li>Unraid will recognize if any drives have been previously used by
Unraid, and when you start the array as part of this procedure the
contents of such disks will be left intact.</li>
<li>There is a checkbox next to the Start button that you can use to say
&#x27;Parity is Valid&#x27;. Do not check this unless you are sure it is the
correct thing to do, or unless advised to do so by an experienced
Unraid user as part of a data recovery procedure.</li>
<li>Removing a data drive from the array will always invalidate parity
unless special action has been taken to ensure the disk being
removed only contains zeroes</li>
<li>Reordering disks after doing the New Config without removing drives
does not invalidate parity1, but it <strong>DOES</strong> invalidate parity2.</li>
</ul>
<p><em>Undoing a reset</em></p>
<p>If for any reason after performing a reset, you wish to undo it, perform
the following steps:</p>
<ol>
<li>Browse to your flash device over the network (SMB)</li>
<li>Open the <strong>Config</strong> folder</li>
<li>Rename the file <em>super.old</em> to <em>super.dat</em></li>
<li>Refresh the browser on the <strong>Main</strong> page and your array
configuration will be restored</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="notifications">Notifications<a class="hash-link" aria-label="Direct link to Notifications" title="Direct link to Notifications" href="/unraid-os/manual/storage-management/#notifications">​</a></h2>
<p>TBD</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="status-reports">Status Reports<a class="hash-link" aria-label="Direct link to Status Reports" title="Direct link to Status Reports" href="/unraid-os/manual/storage-management/#status-reports">​</a></h3>
<p>Unraid can be configured to send you status reports about the state of
the array.</p>
<p>An important point about these reports is:</p>
<ul>
<li>They only tell you if the array currently has any disks disabled or
showing read/write errors.</li>
<li>The status is reset when you reboot the system, so it does not tell
you what the status was in the past.</li>
<li><strong>IMPORTANT</strong>: The status report does not take into account the
SMART status of the drive. You can therefore get a status report
indicating that the array appears to be healthy even though the
SMART information might indicate that a disk might not be too
healthy.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="smart-monitoring">SMART Monitoring<a class="hash-link" aria-label="Direct link to SMART Monitoring" title="Direct link to SMART Monitoring" href="/unraid-os/manual/storage-management/#smart-monitoring">​</a></h2>
<p>Unraid can be configured to report whether SMART attributes for a drive
are changing. The idea is to try and tell you in advance if drives might
be experiencing problems even though they have not yet caused read/write
errors so that you can take pre-emptive action before a problem becomes
serious and thus might potentially lead do data loss. You should have
notifications enabled so that you can see these notifications even when
you are not running the Unraid GUI.</p>
<p>SMART monitoring is currently only supported for SATA drives and is not
available for SAS drives.</p>
<p>Which SMART attributes are monitored can be configured by the user, but
the default ones are:</p>
<ul>
<li>5: Reallocated Sectors count</li>
<li>187: Reported uncorrected errors</li>
<li>188: Command timeout</li>
<li>197: Current / Pending Sector Count</li>
<li>198: Uncorrectable sector count</li>
<li>199: UDMA CRC error count</li>
</ul>
<p>If any of these attributes change value then this will be indicated on
the Dashboard by the icon against the drive turning orange. You can
click on this icon and a menu will appear that allows you to acknowledge
that you have seen the attribute change, and then Unraid will stop
telling you about it unless it changes again.</p>
<p>You can manually see all the current SMART information for a drive by
clicking on its name on the Main tab in the Unraid GUI.</p>
<h1>Pool (cache) Operations</h1>
<p>Prior to Unraid 6.9.0 there was only one pool supported and it was
always called <strong>cache</strong>. Starting with Unraid 6.9.0 multiple pools are
supported and the names of these pools are user defined. When multiple
pools are present then any (or all) of them can have the functionality
that was available with the cache in earlier Unraid releases.</p>
<p>If you are running Unraid 6.9.0 or later then any reference you find in
documentation to cache can be considered as applying to any pool, not
just one that is actually named cache.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-use-a-pool">Why use a Pool?<a class="hash-link" aria-label="Direct link to Why use a Pool?" title="Direct link to Why use a Pool?" href="/unraid-os/manual/storage-management/#why-use-a-pool">​</a></h2>
<p>There are several reasons why a user might want to use a pool in Unraid.</p>
<p>It is worth pointing out that these uses are not mutually exclusive as a
single pool can be used for multiple Use Cases.</p>
<p>UnRaid 6.9 (or later) also supports multiple pools so it is possible to
have individual pools dedicated to specific Use Cases.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cache">Cache<a class="hash-link" aria-label="Direct link to Cache" title="Direct link to Cache" href="/unraid-os/manual/storage-management/#cache">​</a></h3>
<p>The way that Unraid handles parity means that the speed of writing to a
parity protected array is lower than might be expected by the raw speed
of the array disks. If a pool is configured to act as a cache for a User
Share then the perceived speed of writing to the array is that supported
by the pool rather than the speed of writing directly to the array.</p>
<p>A particular User Share can only be associated with one pool at a time,
but it is not necessary for all User Shares to be associated with the
same pool.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="docker-application-storage">Docker application Storage<a class="hash-link" aria-label="Direct link to Docker application Storage" title="Direct link to Docker application Storage" href="/unraid-os/manual/storage-management/#docker-application-storage">​</a></h3>
<p>Docker containers basically consist of 2 parts - the binaries that are
typically stored within the docker.img file that are static and only
updated when the container updates, and the working set that is meant to
be mapped to be external to the docker container (typically as a
container specific subfolder within the appdata share. There are good
reasons to hold both categories on a Pool for several reasons:</p>
<ul>
<li>Writes are much faster than when held on the array as they are not
slowed down by the way in which UnRaid updates parity for a parity
protected array</li>
<li>The working set can be accessed and updated faster when stored on a
Pool.</li>
<li>It is not necessary to have array disks spun up when a container is
accessing its binaries or using its working set.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="vm-vdisks">VM vdisks<a class="hash-link" aria-label="Direct link to VM vdisks" title="Direct link to VM vdisks" href="/unraid-os/manual/storage-management/#vm-vdisks">​</a></h3>
<p>Most VM&#x27;s will have one (or more) vdisk files used to emulate a hard
disk or iso files to emulate a CD-ROM.</p>
<p>Performance of VMs is much better if such files are on a Pool rather
than on an array drive.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="pool-modes">Pool Modes<a class="hash-link" aria-label="Direct link to Pool Modes" title="Direct link to Pool Modes" href="/unraid-os/manual/storage-management/#pool-modes">​</a></h2>
<p>There are two primary modes of operating a pool in Unraid:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="single-device-mode">Single device mode<a class="hash-link" aria-label="Direct link to Single device mode" title="Direct link to Single device mode" href="/unraid-os/manual/storage-management/#single-device-mode">​</a></h3>
<p>When the number of disk slots for the pool is set to one, this is
referred to as running in single device mode. In this mode, you will
have no protection for any data that exists on the pool, which is why
multi-device mode is recommended. However, unlike in multi-device mode,
while in single device mode, you are able to adjust the filesystem for
the cache device to something other than BTRFS. It is for this reason
that there are no special operations for single mode. You can only add
or remove the device from the system.</p>
<p><em>NOTE: If you choose to use a non-BTRFS file system for your pool device
operating in single mode, you will not be able to expand to a
multi-device pool without first reformatting the device with BTRFS. It
is for this reason that BTRFS is the default filesystem for a pool, even
when operating in single device mode.</em></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="multi-device-mode">Multi-Device mode<a class="hash-link" aria-label="Direct link to Multi-Device mode" title="Direct link to Multi-Device mode" href="/unraid-os/manual/storage-management/#multi-device-mode">​</a></h3>
<p>When more than one disk is assigned to the pool, this is referred to as
running in multi-device mode. This mode utilizes a BTRFS specific
implementation of RAID 1 in order to allow for any number of devices to
be grouped together in a pool. Unlike a traditional RAID 1, a BTRFS
RAID1 can mix and match devices of different sizes and speeds and can
even be expanded and contracted as your needs change. To calculate how
much capacity your BTRFS pool will have, check out this handy <a href="http://carfax.org.uk/btrfs-usage/" target="_blank" rel="noopener noreferrer">btrfs
disk usage calculator</a>. Set the
Preset RAID level to RAID-1, select the number of devices you have, and
set the size for each. The tool will automatically calculate how much
space you will have available.</p>
<p>Here are typical operations that are likely to want to carry out on the
pool:</p>
<ul>
<li>Back up the pool to the array</li>
<li>Switch the pool to run in multi-device mode</li>
<li>Add disks</li>
<li>Replace a disk</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="backing-up-the-pool-to-the-array">Backing up the pool to the array<a class="hash-link" aria-label="Direct link to Backing up the pool to the array" title="Direct link to Backing up the pool to the array" href="/unraid-os/manual/storage-management/#backing-up-the-pool-to-the-array">​</a></h2>
<p>The procedure shown assumes that there are at least some dockers and/or
VMs related files on the cache disk, some of these steps are unnecessary
if there aren&#x27;t.</p>
<ol>
<li>Stop all running Dockers/VMs</li>
<li>Settings → VM Manager: disable VMs and click apply</li>
<li>Settings → Docker: disable Docker and click apply</li>
<li>Click on Shares and change to &quot;Yes&quot; all User Shares with &quot;Use
cache disk:&quot; set to &quot;Only&quot; or &quot;Prefer&quot;</li>
<li>Check that there&#x27;s enough free space on the array and invoke the
mover by clicking &quot;Move Now&quot; on the Main page</li>
<li>When the mover finishes check that your pool is empty</li>
</ol>
<p>Note that any files on the pool root will not be moved as they
are not part of any share and will need manual attention</p>
<p>You can then later restore files to the pool by effectively reversing
the above steps:</p>
<ol>
<li>Click on all shares whose content you want on the pool and set &quot;Use
cache:&quot; option to &quot;Only&quot; or &quot;Prefer&quot; as appropriate.</li>
<li>Check that there&#x27;s enough free space on the pool and invoke the
mover by clicking &quot;Move Now&quot; on the Main page</li>
<li>When the mover finishes check that your pool now has the expected
content and that the shares in question no longer have files on the
main array</li>
<li>Settings → Docker: enable Docker and click apply</li>
<li>Settings → VM Manager: enable VMs and click apply</li>
<li>Start any Dockers/VMs that you want to be running</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="switching-the-pool-to-multi-device-mode">Switching the pool to multi-device mode<a class="hash-link" aria-label="Direct link to Switching the pool to multi-device mode" title="Direct link to Switching the pool to multi-device mode" href="/unraid-os/manual/storage-management/#switching-the-pool-to-multi-device-mode">​</a></h2>
<p>If you want a multi-device pool then the only supported format for this
is BTRFS. If it is already in BTRFS format then you can follow the
procedure below for adding an additional drive to a pool</p>
<p>If the cache is NOT in BTRFS format then you will need to do the
following:</p>
<ol>
<li>Use the procedure above for backing up any existing content you want
to keep to the array.</li>
<li>Stop the array</li>
<li>Click on the pool on the Main tab and change the format to be BTRFS</li>
<li>Start the array</li>
<li>The pool should how show as <strong>unmountable</strong> and offer the option to
format the pool.</li>
<li>Confirm that you want to do this and click the format button</li>
<li>When the format finishes you now have a multi-device pool (albeit
with only one drive in it)</li>
<li>If you want additional drives in the pool you can (optionally) do it
now.</li>
<li>Use the restore part of the previous procedure to restore any
content you want on the pool</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="adding-disks-to-a-pool">Adding disks to a pool<a class="hash-link" aria-label="Direct link to Adding disks to a pool" title="Direct link to Adding disks to a pool" href="/unraid-os/manual/storage-management/#adding-disks-to-a-pool">​</a></h2>
<p>Notes:</p>
<ul>
<li>You can only do this if the pool is <strong>already</strong> formatted as BTRFS</li>
</ul>
<p>If it is not then you will need to first follow the steps in the
previous section to create a pool in BTRFS format.</p>
<p>To add disks to the BTRFS pool perform the following steps:</p>
<ol>
<li>Stop the array.</li>
<li>Navigate to the <strong>Main</strong> tab.</li>
<li>Scroll down to the section labeled <em>Pool Devices</em>.</li>
<li>Change the number of <strong>Slots</strong> to be at least as many as the number
of devices you wish to assign.</li>
<li>Assign the devices you wish to the pool slot(s).</li>
<li>Start the array.</li>
<li>Click the checkbox and then the button under <strong>Array Operations</strong> to
format the devices.</li>
</ol>
<p>Make sure that the devices shown are those you expect - you do not
want to accidentally format a device that contains data you want to
keep.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="removing-disks-from-a-multi-device-pool">Removing disks from a multi-device pool<a class="hash-link" aria-label="Direct link to Removing disks from a multi-device pool" title="Direct link to Removing disks from a multi-device pool" href="/unraid-os/manual/storage-management/#removing-disks-from-a-multi-device-pool">​</a></h2>
<p>Notes:</p>
<ul>
<li>You can only do this if your pool is configured for redundancy at
both the data and metadata level.</li>
<li>You can check what raid level your pool is currently set to by
clicking on it on the Main tab and scrolling down to the Balance
Status section.</li>
<li>You can only remove one drive at a time</li>
</ul>
<ol>
<li>Stop the array</li>
<li>Unassign a pool drive.</li>
<li>Start the array</li>
<li>Click on the pool drive</li>
<li>If you still have more than one drive in the pool then you can
simply run a Balance operation</li>
<li>If you only have one drive left in the pool then switch the pool
RAID level to single as described below</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="change-pool-raid-levels">Change Pool RAID Levels<a class="hash-link" aria-label="Direct link to Change Pool RAID Levels" title="Direct link to Change Pool RAID Levels" href="/unraid-os/manual/storage-management/#change-pool-raid-levels">​</a></h2>
<p>BTRFS can add and remove devices online, and freely convert between RAID
levels after the file system has been created.</p>
<p>BTRFS supports raid0, raid1, raid10, raid5, and raid6 (but note that
raid5/6 are still considered experimental so use with care i.e. make
sure you have good backups if using these modes), and it can also
duplicate metadata or data on a single spindle or multiple disks. When
blocks are read in, checksums are verified. If there are any errors,
BTRFS tries to read from an alternate copy and will repair the broken
copy if the alternative copy succeeds.</p>
<p>By default, Unraid creates BTRFS volumes in a pool with data=raid1 and
metadata=raid1 to give redundancy.</p>
<p>For more information about the BTRFS options when using multiple devices
see the <a href="https://btrfs.wiki.kernel.org/index.php/Using_Btrfs_with_Multiple_Devices" target="_blank" rel="noopener noreferrer">BTRFS wiki
article</a>.</p>
<p>You can change the BTRFS raid levels for a pool from the Unraid GUI by:</p>
<ul>
<li>If the array is not started then start it in normal mode</li>
<li>Click on the Pool name on the Main tab</li>
<li>Scroll down to the Balance section</li>
<li>At this point information (including current RAID levels) will be
displayed.</li>
<li><strong>If using UnRaid 6.8.3 or earlier</strong> then add the appropriate
additional parameters added to the <em>Options</em> field.</li>
</ul>
<p>As an example, the following screenshot shows how you might convert
the pool from the RAID1 to the SINGLE profile.</p>
<p><img decoding="async" loading="lazy" src="/assets/images/Btrfs-raid1-0f8821a1511642aa3fe12e6e60be1ae3.jpg" width="1158" height="307" class="img_ev3q"></p>
<ul>
<li><strong>If using UnRaid 6.9.0 or later</strong> this has been made even easier by
giving you a drop-down list of the available options to simply
selecting the one you want</li>
<li>Start the Balance operation.</li>
<li>Wait for the Balance to complete</li>
<li>The new RAID level will now be fully operational.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="replace-a-disk-in-a-pool">Replace a disk in a pool<a class="hash-link" aria-label="Direct link to Replace a disk in a pool" title="Direct link to Replace a disk in a pool" href="/unraid-os/manual/storage-management/#replace-a-disk-in-a-pool">​</a></h2>
<p><strong>Notes:</strong></p>
<ul>
<li>You can only do this if the pool is formatted as BTRFS <strong>AND</strong> it is
set up to be redundant.</li>
<li>You can only replace up to one disk at a time from a pool.</li>
</ul>
<p>To replace a disk in the redundant pool, perform the following steps:</p>
<ol>
<li>Stop the array.</li>
<li>Physically detach the disk from your system you wish to remove.</li>
<li>Attach the replacement disk (must be equal to or larger than the
disk being replaced).</li>
<li>Refresh the Unraid WebGUI when under the <strong>Main</strong> tab.</li>
<li>Select the pool slot that previously was set to the old disk and
assign the new disk to the slot.</li>
<li>Start the array.</li>
<li>If presented with an option to <strong>Format</strong> the device, click the
checkbox and button to do so.</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="remove-a-disk-from-a-pool">Remove a disk from a pool<a class="hash-link" aria-label="Direct link to Remove a disk from a pool" title="Direct link to Remove a disk from a pool" href="/unraid-os/manual/storage-management/#remove-a-disk-from-a-pool">​</a></h2>
<p>There have been times when users have indicated they would like to
remove a disk from a pool they have set up while keeping all the data
intact. This cannot be done from the Unraid GUI but is easy enough to do
from the command line in a console session.</p>
<p><strong>Note</strong>: You need to maintain the minimum number of devices for the
profile in use, i.e., you can remove a device from a 3+ device raid1
pool but you can&#x27;t remove one from a 2 device raid1 pool (unless it&#x27;s
converted to a single profile first), also make sure the remaining
devices have enough space for the current used pool space, or the
removal will fail.</p>
<p>With the array running type on the console:</p>
<p><code>btrfs device remove /dev/sdX1 /mnt/cache</code></p>
<p>Replace X with the correct letter for the drive you want to remove from
the system as shown on the Main tab (don&#x27;t forget the 1 after it).</p>
<p>If the device is encrypted, you will need slightly different syntax:</p>
<p><code>btrfs device remove /dev/mapper/sdX1 /mnt/cache</code></p>
<p>If the drive is an NVMe device, use <code>nvmeXn1p1</code> in place of <code>sdX1</code></p>
<p>Wait for the device to be deleted (i.e., until the command completes and
you get the cursor back).</p>
<p>The device is now removed from the pool, you don&#x27;t need to stop the
array now, but at the next array stop you need to make Unraid forget the
now-deleted member, and to achieve that:</p>
<ul>
<li>Stop the array</li>
<li>Unassign all pool devices</li>
<li>Start the array to make Unraid &quot;forget&quot; the pool config</li>
</ul>
<p>If the docker and/or VMs services were using that pool best to
disable those services before start or Unraid will recreate the
images somewhere else, assuming they are using /mnt/user paths)</p>
<ul>
<li>Stop array (re-enable docker/VM services if disabled above)</li>
<li>Re-assign all pool member except the removed device</li>
<li>Start array</li>
</ul>
<p>Done</p>
<p>You can also remove multiple devices with a single command (as long as
the above rule is observed):</p>
<p><code>btrfs device remove /dev/sdX1 /dev/sdY1 /mnt/cache</code></p>
<p>but in practice this does the same as removing one device, then the
other, as they are still removed one at a time, just one after the other
with no further input from you.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="minimum-free-space-for-a-pool">Minimum Free Space for a Pool<a class="hash-link" aria-label="Direct link to Minimum Free Space for a Pool" title="Direct link to Minimum Free Space for a Pool" href="/unraid-os/manual/storage-management/#minimum-free-space-for-a-pool">​</a></h2>
<p>This setting is used to help avoid the issue of a pool that is being
used for a User Share running out of free space and this then causing
errors to occur. The Minimum Free Space setting for a pool tells Unraid
when to stop putting new files onto the pool for User Shares that have a
<em>Use Cache</em> setting of <strong>Yes</strong> or <strong>Prefer</strong>. Unraid does <strong>not</strong> take
into account file size when selecting a pool and once Unraid has
selected a pool for a file it will not change its mind, and if the file
does not fit you get an out-of-space error. The purpose of the Minimum
Free Space value is that when the free space falls below the level you
set Unraid will start bypassing the pool and writing directly to the
array for any new files. You should therefore set this setting to be
larger than the biggest file you intend to be cached on the pool. In
many ways it is analogous to the setting of the same name for User
Shares but it applies to the pool rather than the array disks. It is
ignored for User Shares which have a <em>Use Cache</em> setting of <strong>Only</strong>,
and is not relevant if the setting is <strong>No</strong>.</p>
<p>For <strong>Unraid 6.8.3</strong> (and earlier) which only supported a single pool
(that was always called cache) this setting can be found under
<em>Settings → Global Share Settings</em>.</p>
<p>For <strong>Unraid 6.9.0</strong> (and later) which supports multiple pools (with the
names being user defined) this setting can be found by clicking on the
pool name on the Main tab.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="moving-files-between-a-pool-and-the-array">Moving files between a Pool and the array<a class="hash-link" aria-label="Direct link to Moving files between a Pool and the array" title="Direct link to Moving files between a Pool and the array" href="/unraid-os/manual/storage-management/#moving-files-between-a-pool-and-the-array">​</a></h2>
<p>A topic that seems to come up with some frequency is what is the process
for getting files that belong to shares (e.g. <em>appdata</em>, <em>system</em>) that
it is normally recommended are held on a pool device for performance
reasons moved to or from the array if the need arises.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="moving-files-from-pool-to-array">Moving files from pool to array<a class="hash-link" aria-label="Direct link to Moving files from pool to array" title="Direct link to Moving files from pool to array" href="/unraid-os/manual/storage-management/#moving-files-from-pool-to-array">​</a></h3>
<p>A typical Use Case for this action is to get files off the pool so that
you can safely perform some action that you are worried might end up
losing any existing contents. The steps are:</p>
<ul>
<li>Disable the Docker and VM services under <em>Settings</em>. This is done to
ensure that they will not hold any files open as that would stop
<em>mover</em> from being able to move them.</li>
<li>Go to the Shares tab and for each share you want to be moved from
the cache to the array ensure that the <em>Use Cache</em> setting is set to
<strong>Yes</strong>.</li>
<li>Go to the Main tab and manually start the <strong>mover</strong> process so that
it starts transferring the files from the cache to the array.</li>
<li>When moves completes the files should now be on the array. You can
validate there are no files left behind by clicking on the
&#x27;folder&#x27; icon at the right side of the cache entry on the Main
tab.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="moving-files-from-array-to-pool">Moving files from array to pool<a class="hash-link" aria-label="Direct link to Moving files from array to pool" title="Direct link to Moving files from array to pool" href="/unraid-os/manual/storage-management/#moving-files-from-array-to-pool">​</a></h3>
<p>The commonest Use Cases for this is when you have either used the above
steps to get files off the cache and now want them back there or if you
have newly added a cache drive and want the files for selected shares
(typically <em>appdata</em> and <em>system</em>) to be moved to the cache. The steps
are:</p>
<ul>
<li>Disable the Docker and VM services under Settings. This is done to
ensure that they will not hold any files open as that would stop
<em>mover</em> from being able to move them.</li>
<li>Go to the Shares tab and for each share you want to be moved from
the array to the cache ensure that the <em>Use Cache</em> setting is set to
<strong>Prefer</strong>.</li>
<li>Go to the Main tab and manually start the <strong>mover</strong> process so that
it starts transferring the files from the array to the cache.</li>
<li>When moves completes the files should now be on the cache.</li>
<li>Re-enable the Docker and/or VM services under <em>Settings</em> (if you use
them).</li>
<li>(optional) Go to the Shares tab and for each share you want all
files always be on the cache set the <em>Use Cache</em> setting to <strong>Only</strong>
to stop any new files for this share being created on the array in
the future.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="multiple-pools">Multiple Pools<a class="hash-link" aria-label="Direct link to Multiple Pools" title="Direct link to Multiple Pools" href="/unraid-os/manual/storage-management/#multiple-pools">​</a></h2>
<p>As of version 6.9, you can create multiple pools and manage them
independently. This feature permits you to define up to 35 named pools,
of up to 30 storage devices per pool.  Pools are created and managed via
the Main page.</p>
<ul>
<li>Note: A pre-6.9.0 cache disk/pool is now simply a pool named
&quot;cache&quot;.  When you upgrade a server which has a cache disk/pool
defined, a backup of <code>config/disk.cfg</code> will be saved to
<code>config/disk.cfg.bak</code>, and then cache device assignment settings are
moved out of <code>config/disk.cfg</code> and into a new file,
<code>config/pools/cache.cfg</code>.  If later you revert back to a pre-6.9.0
Unraid OS release you will lose your cache device assignments and
you will have to manually re-assign devices to cache.  As long as
you reassign the correct devices, data should remain intact.</li>
</ul>
<p>When you create a user share or edit an existing user share, you can
specify which pool should be associated with that share.  The assigned
pool functions identically to the current cache pool operation.</p>
<p>Something to be aware of: when a directory listing is obtained for a
share, the Unraid array disk volumes and all pools which contain that
share are merged in this order:</p>
<blockquote>
<p>pool assigned to share</p>
<p>disk1</p>
<p><code>:</code></p>
<p>disk28</p>
<p>all the <em>other</em> pools in
<a href="https://man7.org/linux/man-pages/man3/strverscmp.3.html" target="_blank" rel="noopener noreferrer">strverscmp()</a>
order.</p>
</blockquote>
<p>A single-device pool may be formatted with either xfs, btrfs, or
(deprecated) reiserfs.  A multiple-device pool may only be formatted
with btrfs.</p>
<p>Note: Something else to be aware of: Let&#x27;s say you have a 2-device
btrfs pool. This will be what btrfs calls &quot;raid1&quot; and what most people
would understand to be &quot;mirrored disks&quot;. Well, this is mostly true in
that the same data exists on both disks but not necessarily at the
block-level.  Now let&#x27;s say you create another pool, and what you do is
un-assign one of the devices from the existing 2-device btrfs pool and
assign it to this pool.  Now you have x2 single-device btrfs pools.
Upon array Start user might understandably assume there are now x2 pools
with exactly the same data.  However, <strong>this is not the case</strong>. Instead,
when Unraid OS sees that a btrfs device has been removed from an
existing multi-device pool, upon array Start it will do a <code>wipefs</code> on
that device so that upon mount it will not be included in the old pool.
This of course effectively deletes all the data on the moved device.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="moving-files-between-pools">Moving files between pools<a class="hash-link" aria-label="Direct link to Moving files between pools" title="Direct link to Moving files between pools" href="/unraid-os/manual/storage-management/#moving-files-between-pools">​</a></h3>
<p>There is no built-in support for moving files between pools. In the
event one wants to do this you can do it using the <strong>mover</strong> application
using the techniques mentioned during earlier by doing it in two steps</p>
<ul>
<li>Move the files from pool1 to the main array</li>
<li>move the files from the array to pool2</li>
</ul>
<p>The alternative is to it manually in which case you can move files
directly between the pools.</p>
<p><strong>Do not forget that if any the files belong to a Docker container
and/or VM then the services must be disabled for the files to be moved
successfully</strong>.</p>
<h1>File System Management</h1>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="selecting-a-file-system-type">Selecting a File System type<a class="hash-link" aria-label="Direct link to Selecting a File System type" title="Direct link to Selecting a File System type" href="/unraid-os/manual/storage-management/#selecting-a-file-system-type">​</a></h2>
<p>Each array drive in an Unraid system is set up as a self-contained file
system. Unraid currently supports the following file system types:</p>
<ul>
<li>
<p><strong>XFS</strong>: This is the default format for array drives on a new
system. It is a well-tried Linux file system and deemed to be the
most robust.</p>
<ul>
<li>XFS is better at recovering from file system corruption than
BTRFS or ZFS (which can happen after unclean shutdowns or system
crashes).</li>
<li>If used on an array drive then each XFS format drive is a single
self-contained file system.</li>
</ul>
</li>
<li>
<p><strong>ZFS</strong>: This is a newer file system introduced with Unraid 6.12
that supports advanced features not available with XFS.</p>
<ul>
<li>It supports detecting file content corruption (often
colloquially known as bit-rot) by internally using checksum
techniques</li>
<li>If used on array drives then each ZFS format drive is an
individual free-standing BTRFS file system.</li>
<li>It can support a single file system spanning multiple drives.
Normally each drive would be of the same size, but if not then
only the amount of space equivalent to that on the smallest
drive will be used.</li>
<li>In multi-drive mode various levels of RAID can be supported. The
default in Unraid for a cache pool is RAID1 so that data is
stored redundantly to protect against drive failure.</li>
<li>It is an option supported when using a cache pool spanning multiple
drives that need to run as a single logical drive as this needs
the multi-drive support.</li>
<li>In multi-drive mode in the cache pool the usable space is always
a multiple of the smallest drive (if they are not the same
size).</li>
<li>It is thought to be better at recovering from file system
corruption than BTRFS, although not as good as XFS.</li>
</ul>
</li>
<li>
<p><strong>BTRFS</strong>: This is a newer file system that supports advanced
features not available with XFS. It is considered not quite as
stable as XFS but many Unraid users have reported in seems as robust
as XFS when used on array drives where each drive is a
self-contained file system. Some of its features are:</p>
<ul>
<li>It supports detecting file content corruption (often
colloquially known as bit-rot) by internally using checksum
techniques.</li>
<li>If used on array drives then each BTRFS format drive is an
individual free-standing BTRFS file system.</li>
<li>It can support a single file system spanning multiple drives,
and in such a case it is not necessary that the drives all be of
the same size. It is better than ZFS at making use of available
space in a multi-drive pool where the drives are of different
sizes.</li>
<li>In multi-drive mode various levels of RAID can be supported
(although these are a BTRFS specific implementation and not
necessarily what one expects). The default in Unraid for a cache
pool is RAID1 so that data is stored redundantly to protect
against drive failure.</li>
<li>It is an option supported when using a cache pool spanning
multiple drives that need to run as a single logical drive as
this needs the multi-drive support.</li>
<li>In multi-drive mode in the cache pool the available space is
always a multiple of the smallest drive size.</li>
</ul>
</li>
<li>
<p><strong>ReiserFS</strong>: This is supported for legacy reasons for those
migrating from earlier versions of Unraid where it was the only
supported file system type.</p>
<ul>
<li>There is only minimal involvement from Linux kernel developers
on maintaining the ReiserFS drivers on new Linux kernel versions
so the chance of a new kernel causing problems with ReiserFS is
higher than for other Linux file system types.</li>
<li><strong>It has a hard limit of 16TB on a ReiserFS file system and
commercial grade hard drives have now reached this limit.</strong></li>
<li>Write performance can degrade significantly as the file system
starts getting full.</li>
<li>It is extremely good at recovering from even extreme levels of
file system corruption</li>
<li><strong>It is now deprecated for use with Unraid and should not be
used by new users.</strong> Support for ReiserFS is due to be removed
from the Linux kernel by 2025 and at that point Unraid will also
likely stop supporting ReiserFS so existing users should be looking
to move off using ReiserFS in their Unraid system.</li>
</ul>
</li>
</ul>
<p>These formats are standard Linux formats and as such any array drive can
easily be removed from the array and read on any Linux system. This can
be very useful in any data recovery scenario. Note, however, that the
initial format needs to be done on the Unraid system as Unraid has
specific requirements around how the disk is partitioned that are
unlikely to be met if the partitioning is not done on Unraid.
Unfortunately, these formats cannot be read as easily on Windows or
macOS systems as these OS do not recognize the file system formats
without additional software being installed that is not freely
obtainable.</p>
<p>A user can use a mixture of these file system types in their Unraid
system without it causing any specific issues. In particular, the Unraid
parity system is file system agnostic as it works at the physical sector
level and is not even aware of the file system that is in use on any
particular drive.</p>
<p>In addition drives can be
<a href="/unraid-os/manual/security/data-encryption/">encrypted</a>. <strong><em>A point to
note about using encryption is that if you get any sort of file system
corruption then encryption can make it harder (and sometimes impossible
to recover data on the corrupted file system.</em></strong></p>
<p>If using a cache pool (i.e multiple drives) then the supported types are
BTRFS or ZFS and the pool is formatted as a single entity. By default,
this will be a version of RAID1 to give redundancy, but other options
can be achieved by running the appropriate <em>btrfs</em> command.</p>
<p>Additional file formats are supported by the <strong>Unassigned Devices</strong> and
<strong>Unassigned Devices Plus</strong> plugins. There can be useful when you have
drives that are to be used for transfer purposes, particularly to
systems that do not support standard Linux formats.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="setting-a-file-system-type">Setting a File System type<a class="hash-link" aria-label="Direct link to Setting a File System type" title="Direct link to Setting a File System type" href="/unraid-os/manual/storage-management/#setting-a-file-system-type">​</a></h2>
<p>The File System type for a new drive can be set in 2 ways:</p>
<ol>
<li>Under <em>Settings → Disk Settings</em> the default type for array drives
and the cache pool can be set.<!-- -->
<ul>
<li>On a new Unraid system this will be XFS for array drives and
BTRFS for the cache.</li>
</ul>
</li>
<li>Explicitly for individual drives by clicking on a drive on the Main
tab (with the array stopped) and selecting a type from those
offered.<!-- -->
<ul>
<li>When a drive is first added the file system type will show as
<strong>auto</strong> which means use the setting specified under
<em>Settings → Disk Settings</em>.</li>
<li>Setting an explicit type over-rides the global setting</li>
<li>The only supported format for a cache containing more than one
drive is BTRFS.</li>
</ul>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="creating-a-file-system-format">Creating a File System (Format)<a class="hash-link" aria-label="Direct link to Creating a File System (Format)" title="Direct link to Creating a File System (Format)" href="/unraid-os/manual/storage-management/#creating-a-file-system-format">​</a></h2>
<p>Before a disk can be used in Unraid then an empty file system of the
desired type needs to be created on the disk. This is the operation
commonly known as &quot;format&quot; and it <strong>erases</strong> any existing content on
the disk.</p>
<p><strong>WARNING:</strong></p>
<p>If a drive has already been formatted by Unraid then if it now shows as
<strong>unmountable</strong> you probably do <strong>NOT</strong> want to format it again unless
you want to <strong>erase</strong> its contents. In such cases, the appropriate
action is usually instead to use the <em>File System check/repair process</em>
detailed later.</p>
<p>The basic process to format a drive once the file system type has been
set is:</p>
<ul>
<li>Start the array</li>
<li>Any drives where Unraid does not recognize the format will be shown
as <strong>unmountable</strong> and there will be an option to format unmountable
drives</li>
<li>Check that <strong>ALL</strong> the drives shown as unmountable are ones you want
to format. You do not want to accidentally format another drive and
erase its contents</li>
<li>Click the check box to say you really want to format the drive.</li>
<li>Carefully read the resulting dialog that outlines the consequences</li>
<li>The <strong>Format</strong> button will now be enabled so if you want to go ahead
with the format click on it.</li>
<li>The format process will start running for the specified disks.<!-- -->
<ul>
<li>If the disk has not previously been used by Unraid then it will
start by rewriting the partition table on the drive to conform
to the standard Unraid expects.</li>
</ul>
</li>
<li>The format should only take a few minutes but if the progress does
not automatically update you might need to refresh the Main tab.</li>
</ul>
<p>Once the format has completed then the drive is ready to start being
used to store files.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="drive-shows-as-unmountable">Drive shows as unmountable<a class="hash-link" aria-label="Direct link to Drive shows as unmountable" title="Direct link to Drive shows as unmountable" href="/unraid-os/manual/storage-management/#drive-shows-as-unmountable">​</a></h2>
<p>A drive can show as <strong>unmountable</strong> in the Unraid GUI for two reasons:</p>
<ul>
<li>
<p>The disk has never been used in Unraid and you have just added is
new a new disk slot in the array. In this case, you want to follow
the format procedure shown above to create a new empty file system
on the drive so it is ready to receive files.</p>
</li>
<li>
<p>File system corruption has occurred. This means that the file system
driver has noticed some inconsistency in the file system control
structures. This is not infrequent if a write to a disk fails for
any reason and Unraid marks the disk as disabled, although it can
occur at other times as well.</p>
</li>
</ul>
<p><strong>Note:</strong> If a disk is showing as both <strong>unmountable and disabled</strong>
(has a red &#x27;x&#x27; against in in the GUI) then the check/repair
process can be carried out on the disk that is being &#x27;emulated&#x27; by
Unraid prior to carrying out any rebuild process. It is always worth
doing the repair before any rebuild as if a disk is showing as
<strong>unmountable</strong> while being emulated then it will also show as
<strong>unmountable</strong> after the rebuild (as all the rebuild process does
is make the physical disk match the emulated one). The process for
repairing a file system is much faster than the rebuild process so
there is not much point in wasting time on a rebuild if the repair
is not going to work. Also if there are any problems running the
repair process on the emulated disk then the physical disk is still
untouched giving a fall back data recovery path.</p>
<p><strong>IMPORTANT</strong>: You do <strong>not</strong> want to format the drive in this case
as this will write an empty file system to the drive update parity
accordingly and you would therefore lose the contents of the drive.</p>
<p>It is worth noting that an Unmountable disk caused by file system
corruption is not something that can be repaired using the parity drive
as it is basically not a result of a write to a disk failing but of
incorrect data being written (apparently successfully) to the data drive
and parity updated accordingly. Such corruption can be due to either a
software issue, or something like bad RAM corrupting the in-memory data
before it is written.</p>
<p>The file system has a level of redundancy in the control structures so
it is normally possible to repair the damage that has been detected.
Therefore when you have an unmountable disk caused by file system
corruption then you want to use the file system check/repair process
documented below to get the disk back into a state where you can mount
it again and see all its data.</p>
<p>If you are at all unsure on the best way to proceed it is often a good
idea to make a post in the forums and attach your system&#x27;s diagnostics
zip file (obtained via Tools → Diagnostics) so you can get feedback on
your issue.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="checking-a-file-system">Checking a File System<a class="hash-link" aria-label="Direct link to Checking a File System" title="Direct link to Checking a File System" href="/unraid-os/manual/storage-management/#checking-a-file-system">​</a></h2>
<p>If a disk that was previously mounting fine suddenly starts showing as
<strong>unmountable</strong> then this normally means that there is some sort of
corruption at the file system level. This most commonly occurs after an
unclean shutdown but could happen any time a write to a drive fails or
if the drive ends up being marked as <strong>disabled</strong> (i.e. with a
red &#x27;,&#x27; in the Unraid GUI). If the drive is marked as disable and
being emulated then the check is run against the emulated drive and not
the physical drive.</p>
<p><strong>IMPORTANT:</strong> At this point, the Unraid GUI will be offering an option
to format unmountable drives. This will <strong>erase</strong> all content on the
drive and <strong>update parity</strong> to reflect this making recovering the data
impossible/very difficult so do <strong>NOT</strong> do this unless you are happy to
lose the contents of the drive.</p>
<p>To recover from file system corruption then one needs to run the tool
that is appropriate to the file system on the disk. Points to note that
users new to Unraid often misunderstand are:</p>
<ul>
<li>Rebuilding a disk does <strong>not</strong> repair file system corruption</li>
<li>If a disk is showing as being emulated then the file system check
and/or repair are run against the emulated drive and not the
physical drive.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="preparing-to-test">Preparing to test<a class="hash-link" aria-label="Direct link to Preparing to test" title="Direct link to Preparing to test" href="/unraid-os/manual/storage-management/#preparing-to-test">​</a></h3>
<p>The first step is to identify the file system of the drive you wish to
test or repair. If you don&#x27;t know for sure, then go to the Main page of
the WebGUI, and click on the name of the drive (Disk 3, Cache, etc).
Look for <strong>File system type</strong>, and you will see the file system format
for your drive (should be <strong>xfs</strong>, <strong>btrfs</strong> or <strong>reiserfs</strong>).</p>
<p>If the file system is <strong>XFS</strong> or <strong>ReiserFS</strong> then you must start the
array in <em>Maintenance mode</em>, by clicking the Maintenance mode check box
before clicking the Start button. This starts the Unraid driver but does
not mount any of the drives.</p>
<p>If the file system is <strong>BTRFS</strong>, then frequently you want to run a
<strong>scrub</strong> rather than a repair as that both checks the BTRFS file system
and can also fix many BTRFS errors. A scrub operation is run with the
array started in Normal mode and NOT in <em>Maintenance</em> mode. If you want
to run a repair then you will need to start the array in <em>Maintenance</em>
mode.</p>
<p><strong>Note:</strong> Details will need to be added for <strong>ZFS</strong> file systems after
Unraid 6.12 is release with ZFS support built in.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="running-the-test-using-the-webgui">Running the Test using the WebGUI<a class="hash-link" aria-label="Direct link to Running the Test using the WebGUI" title="Direct link to Running the Test using the WebGUI" href="/unraid-os/manual/storage-management/#running-the-test-using-the-webgui">​</a></h3>
<p>The process for checking a file system using the Unraid GUI is as
follows:</p>
<ol>
<li>Make sure that you have the array started in the correct mode. If
necessary stop the array and restart in the correct mode by
clicking/unclicking the Maintenance Mode checkbox next to the Start
button.</li>
<li>From the Main screen of the WebGUI, click the name of the disk that
you want to test or repair. For example, if the drive of concern is
Disk 5, then click on <strong>Disk 5</strong>. If it&#x27;s the Cache drive, then
click on <strong>Cache</strong>. If in Maintenance mode then The disks will not
be mounted but the underlying <em>/dev/mdX</em> type devices that
correspond to each <em>diskX</em> in the Unraid GUI will have been created.
This is important as any write operation against one of these &#x27;md&#x27;
type devices will also update parity to reflect that write has
happened.</li>
<li>You should see a page of options for that drive, beginning with
various partition, file system format, and spin down settings.</li>
<li>The section following that is the one you want, titled <strong>Check
Filesystem Status</strong>. There is a box with the 2 words <em>Not available</em>
in it. This is the command output box, where the progress and
results of the command will be displayed. Below that is the
<strong>Check</strong> button that starts the test, followed by the options box
where you can type in options for the test/repair command.</li>
<li>The tool that will be run is shown and the status at this point will
show as <em>Not available</em>. The <em>Options</em> field may include a parameter
that causes the selected tool to run in <em>check-only</em> mode so that
the underlying drive is not actually changed. For more help, click
the <strong>Help</strong> button in the upper right.</li>
<li>Click on the Check button to run the file system check</li>
<li>Information on the check progress is now displayed. You may need to
use the <em>Refresh</em> button to get it to update.</li>
<li>If you are not sure what the results of the check mean you should
copy the progress information so you can ask a question in the
forum. When including this information as part of a forum post
mark them as <em>code</em> (using the <strong>&lt;?&gt;</strong> icon) to preserve the
formatting as otherwise it becomes difficult to read.</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="running-the-test-using-the-command-line">Running the Test using the command line<a class="hash-link" aria-label="Direct link to Running the Test using the command line" title="Direct link to Running the Test using the command line" href="/unraid-os/manual/storage-management/#running-the-test-using-the-command-line">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="xfs-and-reiserfs">XFS and ReiserFS<a class="hash-link" aria-label="Direct link to XFS and ReiserFS" title="Direct link to XFS and ReiserFS" href="/unraid-os/manual/storage-management/#xfs-and-reiserfs">​</a></h4>
<p>You can run the file system check from the command line for ReiserFS and
XFSxfs as shown below if the array is started in Maintenance mode by
using a command of the form:</p>
<p><code>xfs_repair -v /dev/mdX</code></p>
<p>or</p>
<p><code>reiserfsck -v /dev/mdX</code></p>
<p>where X corresponds to the diskX number shown in the Unraid GUI. Using
the /dev/mdX type device will maintain parity. If the file system to be
repaired as an encrypted XFS one then the command needs to be modified
to use the /dev/mapper/mdX device</p>
<p>If you ever need to run a check on a drive that is not part of the array
or if the array is not started then you need to run the appropriate
command from a console/terminal session. As an example for an XFS disk
you would use a command of the form:</p>
<p><code>xfs_repair -v /dev/sdX1</code></p>
<p>where X corresponds to the device identifier shown in the Unraid GUI.
Points to note are:</p>
<ul>
<li>The value of X can change when Unraid is rebooted so make sure it is
correct for the current boot</li>
<li>Note the presence of the &#x27;1&#x27; on the end to indicate the partition
to be checked.</li>
<li>The reason for not doing it this way on array drives is that
although the disk would be repaired parity would be invalidated
which can reduce the chances of recovering a failed drive until
valid parity has been re-established.</li>
<li>if you run this form of the command on an array disk you will
invalidate parity so it is not recommended except in exceptional
circumstances.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="btrfs">BTRFS<a class="hash-link" aria-label="Direct link to BTRFS" title="Direct link to BTRFS" href="/unraid-os/manual/storage-management/#btrfs">​</a></h4>
<p>A BTRFS file systems will automatically check the data as part of
reading it so often there is no need to explicitly run a check. If you
do need to run a check you do it with the array started in <em>Normal</em> mode
using the <strong>scrub</strong> command that is covered in more detail in the
<a href="/unraid-os/manual/storage-management/#scrub">Scrub</a> section.</p>
<p>You can run the file system check from the command line for BTRFS as
shown below if the array is started in Maintenance mode by using
commands of the form:</p>
<p><code>btrfs check --readonly /dev/mdX1</code></p>
<p>where X corresponds to the diskX number shown in the Unraid GUI. Using
the /dev/mdX type device will maintain parity. If the file system to be
repaired is an encrypted XFS one then the command needs to be modified
to use the /dev/mapper/mdX device.</p>
<p>If you ever need to run a check on a drive that is not part of the
array, if the array is not started, or the disk is part of a pool then
you need to run the appropriate command from a console/terminal session.
As an example you would use a command of the form:</p>
<p><code>btrfs check --readonly /dev/sdX1</code></p>
<p>for pools which are outside the Unraid parity scheme</p>
<p>where X corresponds to the device identifier shown in the Unraid GUI.
Points to note are:</p>
<ul>
<li>The value of X can change when Unraid is rebooted so make sure it is
correct for the current boot</li>
<li>Note the presence of the &#x27;1&#x27; on the end to indicate the partition
to be checked.</li>
<li>The reason for not doing it this way on array drives is that
although the disk would be repaired parity would be invalidated
which can reduce the chances of recovering a failed drive until
valid parity has been re-established.</li>
<li>if you run this form of the command on an array disk you will
invalidate parity so it is not recommended except in exceptional
circumstances.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="zfs">ZFS<a class="hash-link" aria-label="Direct link to ZFS" title="Direct link to ZFS" href="/unraid-os/manual/storage-management/#zfs">​</a></h4>
<p>This section should be completed once Unraid 6.12 has been
released with ZFS support included as a standard feature.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="repairing-a-file-system">Repairing a File System<a class="hash-link" aria-label="Direct link to Repairing a File System" title="Direct link to Repairing a File System" href="/unraid-os/manual/storage-management/#repairing-a-file-system">​</a></h2>
<p>You typically run this just after running a check as outlined above, but
if skipping that follow steps 1-4 to get to the point of being ready to
run the repair. It is a good idea to enable the Help built into the GUI
to get more information on this process.</p>
<p>If the drive is marked as disabled and being emulated then the repair is
run against the emulated drive and not the physical drive. It is
frequently done before attempting to rebuild a drive as it is the
contents of the emulated drive that is used by the rebuild process.</p>
<ol>
<li>Remove any parameters from the <em>Options</em> field that would cause the
tool to run in <em>check-only</em> mode.</li>
<li>Add any additional parameters to the <em>Options</em> field required that
are suggested from the check phase. If not sure then ask in the
forum.<!-- -->
<ul>
<li>The Help built into the GUI can provide guidance on what options
might be applicable.</li>
</ul>
</li>
<li>Press the Check button to start the repair process. You can now
periodically use the <em>Refresh</em> button to update the progress
information</li>
<li>If the repair does not complete for any reason then ask in the forum
for advice on how to best proceed if you are not sure.<!-- -->
<ul>
<li>If repairing an XFS formatted drive then it is quite normal for
the <em>xfs_repair</em> process to give you a warning saying you need
to provide the <strong><em>-L</em></strong> option to proceed. Despite this ominous
warning message this is virtually always the right thing to do
and does not result in data loss.</li>
<li>When asking a question in the forum and when including the
output from the repair attempt as part of your post use
<img decoding="async" loading="lazy" alt="Code" src="data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAYABgAAD/4QAiRXhpZgAATU0AKgAAAAgAAQESAAMAAAABAAEAAAAAAAD/2wBDAAIBAQIBAQICAgICAgICAwUDAwMDAwYEBAMFBwYHBwcGBwcICQsJCAgKCAcHCg0KCgsMDAwMBwkODw0MDgsMDAz/2wBDAQICAgMDAwYDAwYMCAcIDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAz/wAARCAAQABADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD9evgP+2t/w0h8UfGF74Z0e0PwN8E29xZS/EK8v/Jt/EOrwShbmLTYtm2awtFjmSa+Z1jaceXEJBFM6+O/s+f8FEvHPgX4L/B7x78bdFt7f4a/GqyOrR+NbeVEj8DXWqahPcaPpeq26RqsFobC5sLSO/3sPtUTJcbDNHI7v2hv+CePjjwT8FvjF4D+CetW9t8OfjVZDSZfBdxEkcfgm61S/httY1TSrhpFWC1Nhc393JY7GH2mFXt9jTSRyew/Hb9ipf2jfif4NsfE2s2v/Cj/AAPb295D8PbOw8m38QavBLutpdSl3kT2NoscLw2SxrG048yUyiKFEAP/2Q==" width="16" height="16" class="img_ev3q"> option to preserve the formatting
as otherwise it becomes difficult to read</li>
</ul>
</li>
<li>If the repair completes without error then stop the array and
restart in normal mode. The drive should now mount correctly.</li>
</ol>
<p>After running a repair you may well find that a <strong>lost+found</strong> folder is
created on the drive with files/folders with cryptic names (this will
then show as a User Share of the same name). These are folders/files for
which the repair process could not determine the name. If you have good
backups then it is often nor worth trying to sort out the contents of
the lost+found folder but instead restore from the backups. If you
really need to sort out the contents then the linux <strong>file</strong> command can
be used on a file to help determine what kind of data is in the file so
you can open it. If there are a lot of content in lost+found it may not
be worth the trouble unless it is important.</p>
<p>If at any point you do not understand what is happening then ask in the
forum.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="preparing-to-repair">Preparing to repair<a class="hash-link" aria-label="Direct link to Preparing to repair" title="Direct link to Preparing to repair" href="/unraid-os/manual/storage-management/#preparing-to-repair">​</a></h3>
<p>If you are going to repair a <strong>BTRFS</strong>, <strong>XFS</strong> or <strong>ReiserFS</strong> file
system then you always want the array to be started in Maintenace mode</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="running-the-repair-using-the-webgui">Running the Repair using the WebGUI<a class="hash-link" aria-label="Direct link to Running the Repair using the WebGUI" title="Direct link to Running the Repair using the WebGUI" href="/unraid-os/manual/storage-management/#running-the-repair-using-the-webgui">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="xfs-and-reiserfs-1">XFS and ReiserFS<a class="hash-link" aria-label="Direct link to XFS and ReiserFS" title="Direct link to XFS and ReiserFS" href="/unraid-os/manual/storage-management/#xfs-and-reiserfs-1">​</a></h4>
<p>The process for repairing a file system using the Unraid GUI is as
follows:</p>
<ol>
<li>Make sure that you have the array started in the correct mode. If
necessary stop the array and restart in the correct mode by
clicking/unclicking the Maintenance Mode checkbox next to the Start
button.</li>
<li>From the Main screen of the WebGUI, click the name of the disk that
you want to test or repair. For example, if the drive of concern is
Disk 5, then click on <strong>Disk 5</strong>. If it&#x27;s the Cache drive, then
click on <strong>Cache</strong>. If in Maintenance mode then The disks will not
be mounted but the underlying <em>/dev/mdX</em> type devices that
correspond to each <em>diskX</em> in the unRaid GUI will have been created.
This is important as any write operation against one of these &#x27;md&#x27;
type devices will also update parity to reflect that write has
happened.</li>
<li>You should see a page of options for that drive, beginning with
various partition, file system format, and spin down settings.</li>
<li>The section following that is the one you want, titled <strong>Check
Filesystem Status</strong>. There is a box with the 2 words <em>Not available</em>
in it. This is the command output box, where the progress and
results of the command will be displayed. Below that is the
<strong>Check</strong> button that starts the repair</li>
<li>This is followed by the options box where you can type in options.
To run a repair you need to remove the <strong>-n</strong> option. If repairing a
XFS system you often get prompted to also use the <strong>-L</strong> option and
if that happens you rerun the repair adding that option here.</li>
<li>The tool that will be run is shown and the status at this point will
show as <em>Not available</em>. The <em>Options</em> field may include a parameter
that causes the selected tool to run in <em>check-only</em> mode so that
the underlying drive is not actually changed. For more help, click
the <strong>Help</strong> button in the upper right.</li>
<li>Click on the Check button to run the file system check</li>
<li>Information on the check progress is now displayed. You may need to
use the <em>Refresh</em> button to get it to update.</li>
<li>If you are not sure what the results of the check means you should
copy the progress information so you can ask a question in the
forum. When including this information as part of a forum post
mark them as <em>code</em> (using the <strong>&lt;?&gt;</strong> icon) to preserve the
formatting as otherwise it becomes difficult to read.</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="btrfs-1">BTRFS<a class="hash-link" aria-label="Direct link to BTRFS" title="Direct link to BTRFS" href="/unraid-os/manual/storage-management/#btrfs-1">​</a></h4>
<p>A lot of the time running the Scrub operation will be able to detect
(and correct if you have a redundant pool) many errors.</p>
<p>In the event that you need more than this you need the array to be
started in Maintenance mode and then the <strong>Check</strong> option can be used to
run the <em>btrfs check</em> program to check file system integrity on the
device.</p>
<p>The <em>Options</em> field is initialized with <em>--readonly</em> which specifies
check-only. If repair is needed, you should run a second Check pass,
setting the <em>Options</em> to <em>--repair</em>; this will permit <em>btrfs check</em> to
fix the file system.</p>
<p>The BTRFS documentation suggests that its <code>--repair</code> option be used only
if you have been advised by &quot;a developer or an experienced user&quot;. As
of August 2022, the SLE documentation recommends using a Live CD,
performing a backup and only using the repair option as a last resort.</p>
<p>After starting a Check, you should Refresh to monitor progress and
status. Depending on how large the file system is, and what errors might
be present, the operation can take <strong>a long time</strong> to finish (hours).
Not much info is printed in the window, but you can verify the operation
is running by observing the read/write counters increasing for the
device on the Main page</p>
<p>There is another tool, named <code>btrfs-restore</code>, that can be used to
recover files from an unmountable filesystem, without modifying the
broken filesystem itself (i.e., non-destructively) but that is not
supported by the Unraid GUI.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="running-the-repair-using-the-command-line">Running the Repair using the command line<a class="hash-link" aria-label="Direct link to Running the Repair using the command line" title="Direct link to Running the Repair using the command line" href="/unraid-os/manual/storage-management/#running-the-repair-using-the-command-line">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="xfs-and-reiserfs-2">XFS and ReiserFS<a class="hash-link" aria-label="Direct link to XFS and ReiserFS" title="Direct link to XFS and ReiserFS" href="/unraid-os/manual/storage-management/#xfs-and-reiserfs-2">​</a></h4>
<p>You can run the file system check from the command line for ReiserFS and
XFS as shown below if the array is started in Maintenance mode by
using a command of the form:</p>
<p><code>xfs_repair /dev/mdX</code></p>
<p>or</p>
<p><code>reiserfsck /dev/mdX</code></p>
<p>where X corresponds to the diskX number shown in the Unraid GUI. Using
the /dev/mdX type device will maintain parity. If the file system to be
repaired is an encrypted XFS one then the command needs to be modified
to use the /dev/mapper/mdX device</p>
<p>If you ever need to run a check on a drive that is not part of the array
or if the array is not started then you need to run the appropriate
command from a console/terminal session. As an example for an XFS disk
you would use a command of the form:</p>
<p><code>xfs_repair /dev/sdX1</code></p>
<p>where X corresponds to the device identifier shown in the Unraid GUI.
Points to note are:</p>
<ul>
<li>The value of X can change when Unraid is rebooted so make sure it is
correct for the current boot</li>
<li>Note the presence of the &#x27;1&#x27; on the end to indicate the partition
to be checked.</li>
<li>The reason for not doing it this way on array drives is that
although the disk would be repaired parity would be invalidated
which can reduce the chances of recovering a failed drive until
valid parity has been re-established.</li>
<li>if you run this form of the command on an array disk you will
invalidate parity so it is not recommended except in exceptional
circumstances.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="btrfs-2">BTRFS<a class="hash-link" aria-label="Direct link to BTRFS" title="Direct link to BTRFS" href="/unraid-os/manual/storage-management/#btrfs-2">​</a></h4>
<p>You can run the file system check from the command line for BTRFS as
shown below if the array is started in Maintenance mode by using a
command of the form:</p>
<p><code>btrfs check --readonly /dev/sdX1</code></p>
<p>where X corresponds to the device identifier shown in the Unraid GUI.
Points to note are:</p>
<ul>
<li>The value of X can change when Unraid is rebooted so make sure it is
correct for the current boot</li>
<li>Note the presence of the &#x27;1&#x27; on the end to indicate the partition
to be checked.</li>
</ul>
<p>In the event that you want to continue to try and actually repair the
system you can run</p>
<p><code>btrfs check --repair /dev/sdX1</code></p>
<p>but you are advised to only do this after getting advice in the forum as
sometimes the <em>--repair</em> option can damage a BTRFS system even further.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="zfs-1">ZFS<a class="hash-link" aria-label="Direct link to ZFS" title="Direct link to ZFS" href="/unraid-os/manual/storage-management/#zfs-1">​</a></h4>
<p>This section should be completed once Unraid 6.12 has been released with
ZFS support included as a standard feature.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="changing-a-file-system-type">Changing a File System type<a class="hash-link" aria-label="Direct link to Changing a File System type" title="Direct link to Changing a File System type" href="/unraid-os/manual/storage-management/#changing-a-file-system-type">​</a></h2>
<p>There may be times when you wish to change the file system type on a
particular drive. The steps are outlined below.</p>
<p>IMPORTANT: These steps will <strong>erase any existing content</strong> on the drive
so make sure you have first copied it elsewhere before attempting to
change the file system type if you do not want to lose it.</p>
<ol>
<li>Stop the array</li>
<li>Click on the drive whose format you want to change</li>
<li>Change the format to the new one you want to use. Repeat if
necessary for each drive to be changed</li>
<li>Start the array</li>
<li>There will now be an option on the main tab to format unmountable
drives and showing what drives these will be. Check that only the
drive(s) you expect show.</li>
<li>Check the box to confirm the format and then press the Format
button.</li>
<li>The format will now start. It typically only takes a few minutes.
There have been occasions where the status does not update but
refreshing the Main tab normally fixes this.</li>
</ol>
<p>If anything appears to go wrong then ask in the forum to add your system
diagnostics zip file (obtained via Tools → Diagnostics) to your post.</p>
<p>Notes:</p>
<ul>
<li>
<p>For SSDs you can erase the current contents using</p>
<p><code>blkdiscard /dev/sdX</code></p>
<p>at the console where &#x27;X&#x27; corresponds to what is currently shown in
the Unraid GUI for the device. Be careful that you get it right as
you do not want to accidentally erase the contents of the wrong
drive.</p>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="converting-to-a-new-file-system-type">Converting to a new File System type<a class="hash-link" aria-label="Direct link to Converting to a new File System type" title="Direct link to Converting to a new File System type" href="/unraid-os/manual/storage-management/#converting-to-a-new-file-system-type">​</a></h2>
<p>There is the special case of changing a file system where you want to
keep the contents of the drive. The commonest reason for doing this is
those users who ran an older version of Unraid where the only supported
file system type was reiserFS (which is now deprecated) and they want to
switch the drive to using either XFS or BTRFS file system instead.
However, there may be users who want to convert between file system
types for other reasons.</p>
<p>In simplistic terms the process is:</p>
<ol>
<li>Copy the data off the drive in question to another location. This
can be elsewhere on the array or anywhere else suitable.<!-- -->
<ul>
<li>You do have to have enough free space to temporarily hold this
data</li>
<li>Many users do such a conversion just after adding a new drive to
the array as this gives them the free space required.</li>
</ul>
</li>
<li>Follow the procedure above for changing the file system type of the
drive. This will leave you with an empty drive that is now in the
correct format but that has no files on it.</li>
<li>Copy the files you saved in step 1 back to this drive</li>
<li>If you have multiple drives that need to be converted then do them
one at a time.</li>
</ol>
<p>This is a time-consuming process as you are copying large amounts of
data. However, most of this is <em>computer time</em> as the user does not need
to be continually present closely watching the actual copying steps.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="reformatting-a-drive">Reformatting a drive<a class="hash-link" aria-label="Direct link to Reformatting a drive" title="Direct link to Reformatting a drive" href="/unraid-os/manual/storage-management/#reformatting-a-drive">​</a></h2>
<p>If by any chance you want to reformat a drive to erase its contents
keeping the existing file system type then many users find that it may
not be obvious how to do this from the Unraid GUI.</p>
<p>The way to do this is to follow the above process for <a href="/unraid-os/manual/storage-management/#changing-a-file-system-type">changing the file
system type</a>
twice. The first time you change it to any other type, and then once it
has been formatted to the new type repeat the process this time setting
the type back to the one you started with.</p>
<p>This process will only take a few minutes, and as you go parity is
updated accordingly.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="reformatting-a-cache-drive">Reformatting a cache drive<a class="hash-link" aria-label="Direct link to Reformatting a cache drive" title="Direct link to Reformatting a cache drive" href="/unraid-os/manual/storage-management/#reformatting-a-cache-drive">​</a></h2>
<p>There may be times when you want to change the format used on the cache
drive (or some similar operation) and preserve as much of its existing
contents as possible. In such cases the recommended way to proceed that
is least likely to go wrong is:</p>
<ol>
<li>Stop array.</li>
<li>Disable docker and VM services under Settings</li>
<li>Start array. If you have correctly disabled these services there
will be NO Docker or VMstab in the GUI.</li>
<li>Set all shares that have files on the cache and currently don&#x27;t
have a Use Cache<!-- -->:Yes<!-- --> to BE Cache<!-- -->:Yes<!-- -->. Make a note of which shares
you changed and what setting they had before the change</li>
<li>Run mover from the Main tab; wait for completion (which can take
some time to complete if there are a lot of files); check cache
drive contents, should be empty. If it&#x27;s not, STOP, post
diagnostics, and ask for help.</li>
<li>Stop array.</li>
<li>Set cache drive desired format to XFS or BTRFS, if you only have a
single cache disk and are keeping that configuration, then XFS is
the recommended format. XFS is only available as a selection if
there is only 1 (one) cache slot shown while the array is stopped.</li>
<li>Start array.</li>
<li>Verify that the cache drive and ONLY the cache drive shows
unformatted. Select the checkbox saying you are sure, and format the
drive.</li>
<li>Set any shares that you changed to be Cache: Yes earlier to Cache:
Prefer if they were originally Cache: Only or Cache: Prefer. If any
were Cache: No, set them back that way.</li>
<li>Run mover from the Main tab; wait for completion; check cache drive
contents which should be back the way it was.</li>
<li>Change any share that was set to Use Cache<!-- -->:Only<!-- --> back to that option</li>
<li>Stop array.</li>
<li>Enable docker and VM services.</li>
<li>Start array</li>
</ol>
<p>There are other alternative procedures that might be faster if you are
Linux aware, but the one shown above is the one that has proved most
likely to succeed without error for the average Unraid user.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="btrfs-operations">BTRFS Operations<a class="hash-link" aria-label="Direct link to BTRFS Operations" title="Direct link to BTRFS Operations" href="/unraid-os/manual/storage-management/#btrfs-operations">​</a></h2>
<p>If you want more information BTRFS then the
<a href="https://en.wikipedia.org/wiki/Btrfs" target="_blank" rel="noopener noreferrer">Wikipedia BTRFS article</a> is a
good place to start</p>
<p>There are a number of operations that are specific to BTRFS formatted
drives that do not have a direct equivalent in the other formats.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="balance">Balance<a class="hash-link" aria-label="Direct link to Balance" title="Direct link to Balance" href="/unraid-os/manual/storage-management/#balance">​</a></h3>
<p>Unlike most conventional filesystems, BTRFS uses a two-stage allocator.
The first stage allocates large regions of space known as chunks for
specific types of data, then the second stage allocates blocks like a
regular filesystem within these larger regions. There are three
different types of chunks:</p>
<ul>
<li>Data Chunks: These store regular file data.</li>
<li>Metadata Chunks: These store metadata about files, including among
other things timestamps, checksums, file names, ownership,
permissions, and extended attributes.</li>
<li>System Chunks: These are a special type of chunk which stores data
about where all the other chunks are located.</li>
</ul>
<p>Only the type of data that the chunk is allocated for can be stored in
that chunk. The most common case these days when you get a -ENOSPC error
on BTRFS is that the filesystem has run out of room for data or metadata
in existing chunks, and can&#x27;t allocate a new chunk. You can verify that
this is the case by running btrfs fi df on the filesystem that threw the
error. If the Data or Metadata line shows a Total value that is
significantly different from the Used value, then this is probably the
cause.</p>
<p>What btrfs balance does is to send things back through the allocator,
which results in space usage in the chunks being compacted. For example,
if you have two metadata chunks that are both 40% full, a balance will
result in them becoming one metadata chunk that&#x27;s 80% full. By
compacting space usage like this, the balance operation is then able to
delete the now-empty chunks and thus frees up room for the allocation of
new chunks. If you again run btrfs fi df after you run the balance, you
should see that the Total and Used values are much closer to each other,
since balance deleted chunks that weren&#x27;t needed anymore.</p>
<p>The BTRFS balance operation can be run from the Unraid GUI by clicking
on the drive on the Main tab and running scrub from the resulting
dialog. The current status information for the volume is displayed. You
can optionally add parameters to be passed to the balance operation and
then start the scrub by pressing the Balance button.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="scrub">Scrub<a class="hash-link" aria-label="Direct link to Scrub" title="Direct link to Scrub" href="/unraid-os/manual/storage-management/#scrub">​</a></h3>
<p>Scrubbing involves reading all the data from all the disks and verifying
checksums. If any values are not correct and you have a redundant BTRFS
pool then the data can be corrected by reading a good copy of the block
from another drive. The scrubbing code also scans on read automatically.
It is recommended that you scrub high-usage file systems <strong>once a week</strong>
and all other file systems <strong>once a month</strong>.</p>
<p>You can initiate a check of the entire file system by triggering a file
system scrub job. The scrub job scans the entire file system for
integrity. It automatically attempts to report and repair any bad blocks
that it finds along the way. Instead of going through the entire disk
drive, the scrub job deals only with data that is actually allocated.
Depending on the allocated disk space, this is much faster than
performing an entire surface scan of the disk.</p>
<p>The BTRFS scrub operation can be run from the Unraid GUI by clicking on
the drive on the Main tab and running scrub from the resulting dialog.</p>
<h1>Unassigned Drives</h1>
<p>Unassigned drives are drives that are present in the server running
Unraid that have not been added to the array or to a cache pool.</p>
<p><em>It is important to note that all such drives that are plugged into the
server at the point you start the array count towards the Unraid
Attached Devices license limits.</em></p>
<p>Typical uses for such drives are:</p>
<ul>
<li>Plugging in removable drives for the purposes of transferring files
or backing up drives.</li>
<li>Having drives dedicated to specific use (such as running VMs) where
you want higher performance than can be achieved by using array
drives.</li>
</ul>
<p>It is strongly recommended that you install the Unassigned Devices (UD)
plugins via the <strong>Apps</strong> tab if you want to use Unassigned Drives on
your system. There are 2 plugins available:</p>
<ol>
<li>The basic <strong>Unassigned Devices</strong> plugin provides support for file
system types supported as standard in Unraid.</li>
<li>The <strong>Unassigned Devices Plus</strong> plugin extends the file system
support to include options such as ExFat ant HFS+.</li>
</ol>
<p>You should look at the Unassigned Devices support
<a href="https://forums.unraid.net/topic/92462-unassigned-devices-managing-disk-drives-and-remote-shares-outside-of-the-unraid-array/" target="_blank" rel="noopener noreferrer">thread</a>
for these plugins to get more information about the very
extensive facilities offered and guidance on how to use them.</p>
<h1>Performance</h1>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="array-write-modes">Array Write Modes<a class="hash-link" aria-label="Direct link to Array Write Modes" title="Direct link to Array Write Modes" href="/unraid-os/manual/storage-management/#array-write-modes">​</a></h2>
<p>Unraid maintains real-time parity and the performance of writing to the
parity protected array in Unraid is strongly affected by the method that
is used to update parity.</p>
<p>There are fundamentally 2 methods supported:</p>
<ul>
<li>Read/Modify/Write</li>
<li>Turbo Mode (also known as <em>reconstruct write</em>)</li>
</ul>
<p>These are discussed in more detail below to help users decide which
modes are appropriate to how they currently want their array to operate.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="setting-the-write-mode">Setting the Write mode<a class="hash-link" aria-label="Direct link to Setting the Write mode" title="Direct link to Setting the Write mode" href="/unraid-os/manual/storage-management/#setting-the-write-mode">​</a></h3>
<p>The write mode is set by going <em>Settings → Disk Settings</em>, and look for
the <strong>Tunable (md_write_method</strong>) setting. The 3 options are:</p>
<ul>
<li><strong>Auto</strong>: Currently this operates just like setting the
read/modify/write option but is reserved for future enhancement</li>
<li><strong>read/modify/write</strong></li>
<li><strong>reconstruct write</strong> (a.k.a.Turbo write)</li>
</ul>
<p>To change it, click on the option you want, then the Apply button. The
effect should be immediate so you can change it at any time</p>
<p>The different modes and their implications are discussed in more detail
below</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="readmodifywrite-mode">Read/Modify/Write mode<a class="hash-link" aria-label="Direct link to Read/Modify/Write mode" title="Direct link to Read/Modify/Write mode" href="/unraid-os/manual/storage-management/#readmodifywrite-mode">​</a></h3>
<p>Historically, Unraid has used the &quot;read/modify/write&quot; method to update
parity and to keep parity correct for all data drives.</p>
<p>Say you have a block of data to write to a drive in your array, and
naturally you want parity to be updated too. In order to know how to
update parity for that block, you have to know what is the difference
between this new block of data and the existing block of data currently
on the drive. So you start by reading in the existing block and
comparing it with the new block. That allows you to figure out what is
different, so now you know what changes you need to make to the parity
block, but first, you need to read in the existing parity block. So you
apply the changes you figured out to the parity block, resulting in a
new parity block to be written out. Now you want to write out the new
data block, and the parity block, but the drive head is just past the
end of the blocks because you just read them. So you have to wait a long
time (in computer time) for the disk platters to rotate all the way back
around until they are positioned to write to that same block. That
platter rotation time is the part that makes this method take so long.
It&#x27;s the main reason why parity writes are so much slower than regular
writes.</p>
<p>To summarize, for the &quot;read/modify/write&quot; method, you need to:</p>
<ul>
<li>read in the parity block and read in the existing data block (can be
done simultaneously)</li>
<li>compare the data blocks, then use the difference to change the
parity block to produce a new parity block (very short)</li>
<li>wait for platter rotation (very long!)</li>
<li>write out the parity block and write out the data block (can be done
simultaneously)</li>
</ul>
<p>That&#x27;s 2 reads, a calc, a long wait, and 2 writes.</p>
<p>The advantages of this approach are:</p>
<ul>
<li>Only the parity drive(s) and the drive being updated need to be spun
up.</li>
<li>Minimises power usage as array drives can be kept spun down when not
being accessed</li>
<li>Does not require all the other array drives to be working perfectly</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="turbo-write-mode">Turbo write mode<a class="hash-link" aria-label="Direct link to Turbo write mode" title="Direct link to Turbo write mode" href="/unraid-os/manual/storage-management/#turbo-write-mode">​</a></h3>
<p>More recently Unraid introduced the Turbo write mode (often called
&quot;reconstruct write&quot;)</p>
<p>We start with that same block of new data to be saved, but this time we
don&#x27;t care about the existing data or the existing parity block. So we
can immediately write out the data block, but how do we know what the
parity block should be? We issue a read of the same block on all of the
<em>other</em> data drives, and once we have them, we combine all of them
plus our new data block to give us the new parity block, which we then
write out! Done!</p>
<p>To summarize, for the &quot;reconstruct write&quot; method, you need to:</p>
<ul>
<li>write out the data block while simultaneously reading in the data
blocks of all other data drives</li>
<li>calculate the new parity block from all of the data blocks,
including the new one (very short)</li>
<li>write out the parity block</li>
</ul>
<p>That&#x27;s a write and a bunch of simultaneous reads, a calc, and a write,
but no platter rotation wait! The upside is it can be much faster.</p>
<p>The downside is:</p>
<ul>
<li>ALL of the array drives must be spinning, because they ALL are
involved in EVERY write.</li>
<li>Increased power draw due to the need to keep all drives spinning</li>
<li>All drives must be reading without error.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ramifications">Ramifications<a class="hash-link" aria-label="Direct link to Ramifications" title="Direct link to Ramifications" href="/unraid-os/manual/storage-management/#ramifications">​</a></h3>
<p>So what are the ramifications of this?</p>
<ul>
<li>For some operations, like parity checks and parity builds and drive
rebuilds, it doesn&#x27;t matter, because all of the drives are spinning
anyway.</li>
<li>For large write operations, like large transfers to the array, it
can make a big difference in speed!</li>
<li>For a small write, especially at an odd time when the drives are
normally sleeping, all of the drives have to be spun up before the
small write can proceed.</li>
<li>And what about those little writes that go on in the background,
like file system housekeeping operations? EVERY write at any time
forces EVERY array drive to spin up. So you are likely to be
surprised at odd times when checking on your array, and expecting
all of your drives to be spun down, and finding every one of them
spun up, for no discernible reason.</li>
<li>So one of the questions to be faced is, how do you want your various
write operations to be handled. Take a small scheduled backup of
your phone at 4 in the morning. The backup tool determines there&#x27;s
a new picture to back up, so tries to write it to your Unraid
server. If you are using the old method, the data drive and the
parity drive have to spin up, then this small amount of data is
written, possibly taking a couple more seconds than Turbo write
would take. It&#x27;s 4am, do you care? If you were using Turbo write,
then all of the drives will spin up, which probably takes somewhat
longer spinning them up than any time saved by using Turbo write to
save that picture (but a couple of seconds faster in the save).
Plus, all of the drives are now spinning, uselessly.</li>
<li>Another possible problem if you were in Turbo mode, and you are
watching a movie streaming to your player, then a write kicks into
the server and starts spinning up ALL of the drives, causing that
well-known pause and stuttering in your movie. Who wants to deal
with the whining that starts then?</li>
</ul>
<p>Currently, you only have the option to use the old method or the new
(currently the Auto option means the old method). The plan is to add the
true Auto option that will use the old method by default, *unless* all
of the drives are currently spinning. If the drives are all spinning,
then it slips into Turbo. This should be enough for many users. It would
normally use the old method, but if you planned a large transfer or a
bunch of writes, then you would spin up all of the drives - and enjoy
faster writing.</p>
<p>The auto method has the potential of the system automatically
switching modes depending on current array activity but this has not
happened so far. The problem is knowing when a drive is spinning, and
being able to detect it without noticeably affecting write performance,
ruining the very benefits we were trying to achieve. If on every write
you have to query each drive for its status, then you will noticeably
impact I/O performance. So to maintain good performance, you need
another function working in the background keeping near-instantaneous
track of spin status, and providing a single flag for the writer to
check, whether they are all spun up or not, to know which method to use.</p>
<p>Many users would like tighter and smarter control of which write mode is
in use. There is currently no official way of doing this but you could
try searching for &quot;Turbo Write&quot; on the Apps tab for unofficial ways to
get better control.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="using-a-cache-drive">Using a Cache Drive<a class="hash-link" aria-label="Direct link to Using a Cache Drive" title="Direct link to Using a Cache Drive" href="/unraid-os/manual/storage-management/#using-a-cache-drive">​</a></h3>
<p>It is possible to use a Cache Drive/Pool to improve the <strong>perceived</strong>
speed of writing to the array. This can be done on a share-by-share
basis using the Use Cache setting available for each share by clicking
on the share name on the Shares tab in the GUI. It is important to
realize that using the cache has not really sped up writing files to the
array - it is just that such writes now occur when the user is not
watching them</p>
<p>Points to note are:</p>
<ul>
<li>The <strong>Yes</strong> setting for <em>Use Cache</em> causes new files for the share
to initially be written to the cache and later moved to the parity
protected array when <em>mover</em> runs.</li>
<li>Writes to the cache run at the full speed the cache is capable of.</li>
<li>It is not uncommon to use SSDs in the cache to get maximum
performance.</li>
<li>Moves from cache to array are still comparatively slow but since
mover is normally scheduled to run when the system is otherwise idle
this is not visible to the end-user.</li>
<li>There is a <strong>Minimum Free Space</strong> setting under <em>Settings → Global
Share settings</em> and if the free space on the cache falls below this
value Unraid will stop trying to write new files to the cache. Since
when Unraid first creates a file it does not know the final size it
is recommended that the value for this setting should be as large
(or larger) as the biggest file you expect to write to the share as
you want to stop Unraid selecting the cache for a file that will not
fit in the space available. This will stop the write failing with an
&#x27;out of space&#x27; error when the free space gets exhausted.</li>
<li>If there is not sufficient free space on the cache then writes will
start by-passing the cache and revert to the speeds that would be
obtained when not using the cache.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="read-modes">Read Modes<a class="hash-link" aria-label="Direct link to Read Modes" title="Direct link to Read Modes" href="/unraid-os/manual/storage-management/#read-modes">​</a></h2>
<p>Normally read performance is determined by the maximum speed that a file
can be read off a drive. Unlike some other forms of RAID an Unraid
system does not utilize striping techniques to improve performance as
every file is constrained to a single drive.</p>
<p>If a disk is marked as disabled and being emulated then Unraid needs to
reconstruct its contents on the fly by reading the appropriate sectors
of all the good drives and the parity drive(s). In such a case the read
performance is going to be determined primarily by the slowest drives in
the system.</p>
<p>It is also worth emphasizing that if there is any array operation going
on such as a parity check or a disk rebuild then read performance will
be degraded significantly due to drive head movements caused by disk
contention between the two operations.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="cache-pools">(Cache) Pools<a class="hash-link" aria-label="Direct link to (Cache) Pools" title="Direct link to (Cache) Pools" href="/unraid-os/manual/storage-management/#cache-pools">​</a></h2>
<p>Unraid supports the use of (cache pools) that are separate from the main
array and work differently from a performance perspective and these
should be considered when performance is a prime criteria. If a pool
consists of multiple drives then Unraid mandates that is is formatted
using the BTRFS file system.</p>
<p>BTRFS supports a variety of RAID profiles and these will perform more
like a traditional RAID system giving much higher throughput than the
main Unraid array.</p>
<p>Recovery after drive failure tends to be harder and more prone to lead
to data loss which is one disadvantage of using pools for everything.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/unraid/docs/tree/main/docs/unraid-os/manual/storage-management.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/unraid-os/manual/users/reset-password/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Resetting your Unraid password</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/unraid-os/manual/docker-management/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Docker Management</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#help-i-cant-start-my-array">Help! I can&#39;t start my array!</a><ul><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#too-many-disks-missing-from-the-array">Too many disks missing from the array</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#too-many-attached-devices">Too many attached devices</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#invalid-or-missing-key">Invalid or missing key</a><ul><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#missing-key">Missing key</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#expired-trial">Expired trial</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#blacklisted-usb-flash-device">Blacklisted USB flash device</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#cannot-contact-key-server">Cannot contact key-server</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#this-unraid-server-os-release-has-been-withdrawn">This Unraid Server OS release has been withdrawn</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#adding-disks">Adding disks</a><ul><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#configuring-disks">Configuring Disks</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#clear-v-pre-clear">Clear v Pre-Clear</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#data-disks">Data Disks</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#parity-disks">Parity Disks</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#upgrading-parity-disks">Upgrading parity disk(s)</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#replacing-disks">Replacing disks</a><ul><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#replacing-a-disk-to-increase-capacity">Replacing a disk to increase capacity</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#replacing-faileddisabled-disks">Replacing failed/disabled disk(s)</a><ul><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#array-tolerance-to-disk-failure-events">Array Tolerance to Disk Failure Events</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#what-is-a-failed-disabled-drive">What is a &#39;failed&#39; (disabled) drive</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#normal-replacement">Normal replacement</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#rebuilding-a-drive-onto-itself">Rebuilding a drive onto itself</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#parity-swap">Parity Swap</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#a-disk-failed-while-i-was-rebuilding-another">A disk failed while I was rebuilding another</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#removing-disks">Removing disks</a><ul><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#removing-parity-disks">Removing parity disk(s)</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#removing-data-disks">Removing data disk(s)</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#alternative-method">Alternative method</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#notes-1">Notes</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#checking-array-devices">Checking array devices</a><ul><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#parity-check">Parity check</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#read-check">Read check</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#check-history">Check history</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#spin-up-and-down-disks">Spin up and down disks</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#reset-the-array-configuration">Reset the array configuration</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#notifications">Notifications</a><ul><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#status-reports">Status Reports</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#smart-monitoring">SMART Monitoring</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#why-use-a-pool">Why use a Pool?</a><ul><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#cache">Cache</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#docker-application-storage">Docker application Storage</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#vm-vdisks">VM vdisks</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#pool-modes">Pool Modes</a><ul><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#single-device-mode">Single device mode</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#multi-device-mode">Multi-Device mode</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#backing-up-the-pool-to-the-array">Backing up the pool to the array</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#switching-the-pool-to-multi-device-mode">Switching the pool to multi-device mode</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#adding-disks-to-a-pool">Adding disks to a pool</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#removing-disks-from-a-multi-device-pool">Removing disks from a multi-device pool</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#change-pool-raid-levels">Change Pool RAID Levels</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#replace-a-disk-in-a-pool">Replace a disk in a pool</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#remove-a-disk-from-a-pool">Remove a disk from a pool</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#minimum-free-space-for-a-pool">Minimum Free Space for a Pool</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#moving-files-between-a-pool-and-the-array">Moving files between a Pool and the array</a><ul><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#moving-files-from-pool-to-array">Moving files from pool to array</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#moving-files-from-array-to-pool">Moving files from array to pool</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#multiple-pools">Multiple Pools</a><ul><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#moving-files-between-pools">Moving files between pools</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#selecting-a-file-system-type">Selecting a File System type</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#setting-a-file-system-type">Setting a File System type</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#creating-a-file-system-format">Creating a File System (Format)</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#drive-shows-as-unmountable">Drive shows as unmountable</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#checking-a-file-system">Checking a File System</a><ul><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#preparing-to-test">Preparing to test</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#running-the-test-using-the-webgui">Running the Test using the WebGUI</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#running-the-test-using-the-command-line">Running the Test using the command line</a><ul><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#xfs-and-reiserfs">XFS and ReiserFS</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#btrfs">BTRFS</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#zfs">ZFS</a></li></ul></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#repairing-a-file-system">Repairing a File System</a><ul><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#preparing-to-repair">Preparing to repair</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#running-the-repair-using-the-webgui">Running the Repair using the WebGUI</a><ul><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#xfs-and-reiserfs-1">XFS and ReiserFS</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#btrfs-1">BTRFS</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#running-the-repair-using-the-command-line">Running the Repair using the command line</a><ul><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#xfs-and-reiserfs-2">XFS and ReiserFS</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#btrfs-2">BTRFS</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#zfs-1">ZFS</a></li></ul></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#changing-a-file-system-type">Changing a File System type</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#converting-to-a-new-file-system-type">Converting to a new File System type</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#reformatting-a-drive">Reformatting a drive</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#reformatting-a-cache-drive">Reformatting a cache drive</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#btrfs-operations">BTRFS Operations</a><ul><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#balance">Balance</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#scrub">Scrub</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#array-write-modes">Array Write Modes</a><ul><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#setting-the-write-mode">Setting the Write mode</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#readmodifywrite-mode">Read/Modify/Write mode</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#turbo-write-mode">Turbo write mode</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#ramifications">Ramifications</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#using-a-cache-drive">Using a Cache Drive</a></li></ul></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#read-modes">Read Modes</a></li><li><a class="table-of-contents__link toc-highlight" href="/unraid-os/manual/storage-management/#cache-pools">(Cache) Pools</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Documentation</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/">Home</a></li><li class="footer__item"><a href="https://github.com/unraid/docs" target="_blank" rel="noopener noreferrer" class="footer__link-item">Contribute on Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://forums.unraid.net" target="_blank" rel="noopener noreferrer" class="footer__link-item">Forums<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discord.unraid.net/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://unraid.net" target="_blank" rel="noopener noreferrer" class="footer__link-item">Unraid Home<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://connect.myunraid.net" target="_blank" rel="noopener noreferrer" class="footer__link-item">Unraid Connect<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://newsletter.unraid.net" target="_blank" rel="noopener noreferrer" class="footer__link-item">Newsletter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://unraid.net/blog" target="_blank" rel="noopener noreferrer" class="footer__link-item">Blog<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="margin-bottom--sm"><img src="/img/un-mark-gradient.svg" alt="Unraid Logo" class="footer__logo themedComponent_mlkZ themedComponent--light_NVdE" width="100"><img src="/img/un-mark-gradient.svg" alt="Unraid Logo" class="footer__logo themedComponent_mlkZ themedComponent--dark_xIcU" width="100"></div><div class="footer__copyright"><small>Copyright &copy; 2005-2024 Lime Technology, Inc.<br>Unraid&reg; is a registered trademark of Lime Technology, Inc.</small></div></div></div></footer></div>
</body>
</html>